# ============================================
# CONFIGURATION ENVIRONNEMENT VLLM - TEMPLATE
# ============================================
# 
# ⚠️ IMPORTANT SÉCURITÉ:
# Ce fichier est un TEMPLATE SEULEMENT. Copiez-le vers .env et remplacez
# les valeurs par vos vrais tokens/clés. Le fichier .env ne doit JAMAIS
# être commité dans Git (il est dans .gitignore).
#
# Commande de setup initiale:
#   cp myia_vllm/.env.example myia_vllm/.env
#   # Puis éditer .env avec vos vraies valeurs
#

# ============================================
# HUGGING FACE CONFIGURATION
# ============================================
# Token requis pour télécharger les modèles depuis Hugging Face Hub
# Obtenez votre token sur: https://huggingface.co/settings/tokens
# Format: hf_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX (37 caractères après "hf_")
HUGGING_FACE_HUB_TOKEN=hf_YOUR_TOKEN_HERE_37_CHARACTERS

# ============================================
# CLÉS API SERVICES VLLM
# ============================================
# Clés API pour sécuriser l'accès aux différents services vLLM
# Générez des clés aléatoires sécurisées (32 caractères alphanumériques recommandés)
# Exemple de génération: pwsh -c "[guid]::NewGuid().ToString('N').ToUpper()"
VLLM_API_KEY_MICRO=YOUR_MICRO_API_KEY_HERE_32_CHARS
VLLM_API_KEY_MINI=YOUR_MINI_API_KEY_HERE_32_CHARS
VLLM_API_KEY_MEDIUM=YOUR_MEDIUM_API_KEY_HERE_32_CHARS

# ============================================
# PORTS SERVICES
# ============================================
# Ports d'écoute pour chaque service (défaut si non spécifié dans docker-compose)
VLLM_PORT_MICRO=5000
VLLM_PORT_MINI=5001
VLLM_PORT_MEDIUM=5002

# ============================================
# URLS EXTERNES (OPTIONNEL)
# ============================================
# URLs publiques des APIs si exposées via reverse proxy/load balancer
VLLM_URL_MICRO=https://api.micro.your-domain.com/
VLLM_URL_MINI=https://api.mini.your-domain.com/
VLLM_URL_MEDIUM=https://api.medium.your-domain.com/

# ============================================
# CONFIGURATION GPU
# ============================================
# Numéros des GPUs à utiliser pour chaque service (séparés par virgules)
# Vérifier avec: nvidia-smi
CUDA_VISIBLE_DEVICES_MICRO=2
CUDA_VISIBLE_DEVICES_MINI=2
CUDA_VISIBLE_DEVICES_MEDIUM=0,1

# Utilisation mémoire GPU (0.0 à 1.0)
# Recommandé: 0.90-0.95 pour production
GPU_MEMORY_UTILIZATION_MICRO=0.90
GPU_MEMORY_UTILIZATION_MINI=0.90
GPU_MEMORY_UTILIZATION_MEDIUM=0.95

# ============================================
# MODÈLES PAR DÉFAUT
# ============================================
# Chemins ou identifiants Hugging Face des modèles à charger
VLLM_MODEL_MICRO=Orion-zhen/Qwen3-1.7B-AWQ
VLLM_MODEL_MINI=Qwen/Qwen3-8B-AWQ
VLLM_MODEL_MEDIUM=Qwen/Qwen3-32B-AWQ

# ============================================
# OPTIMISATIONS PERFORMANCE
# ============================================
# Nombre de threads OpenMP (1 recommandé pour éviter la contention)
OMP_NUM_THREADS=1

# Fuseau horaire
TZ=Europe/Paris

# Cache Hugging Face (optionnel, chemin personnalisé)
# Par défaut: ~/.cache/huggingface/hub
# HF_CACHE_PATH=/path/to/custom/cache

# ============================================
# PARAMÈTRES AVANCÉS (OPTIONNEL)
# ============================================
# Dtype par défaut pour les calculs
# DTYPE_MICRO=half
# DTYPE_MINI=half
# DTYPE_MEDIUM=half

# Longueur maximale de contexte (tokens)
# MAX_MODEL_LEN_MICRO=32768
# MAX_MODEL_LEN_MINI=65536
# MAX_MODEL_LEN_MEDIUM=131072

# Backend d'attention (FLASHINFER recommandé pour Qwen3)
# VLLM_ATTENTION_BACKEND=FLASHINFER

# Autoriser contextes plus longs que définition du modèle
# VLLM_ALLOW_LONG_MAX_MODEL_LEN=1