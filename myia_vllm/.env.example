# ============================================
# CONFIGURATION ENVIRONNEMENT VLLM - TEMPLATE
# ============================================
# 
# ⚠️ IMPORTANT SÉCURITÉ:
# Ce fichier est un TEMPLATE SEULEMENT. Copiez-le vers .env et remplacez
# les valeurs par vos vrais tokens/clés. Le fichier .env ne doit JAMAIS
# être commité dans Git (il est dans .gitignore).
#
# Commande de setup initiale:
#   cp myia_vllm/.env.example myia_vllm/.env
#   # Puis éditer .env avec vos vraies valeurs
#

# ============================================
# HUGGING FACE CONFIGURATION
# ============================================
# Token requis pour télécharger les modèles depuis Hugging Face Hub
# Obtenez votre token sur: https://huggingface.co/settings/tokens
# Format: hf_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX (37 caractères après "hf_")
HUGGING_FACE_HUB_TOKEN=hf_YOUR_TOKEN_HERE_37_CHARACTERS

# ============================================
# CLÉS API SERVICES VLLM
# ============================================
# Clés API pour sécuriser l'accès aux différents services vLLM
# Générez des clés aléatoires sécurisées (32 caractères alphanumériques recommandés)
# Exemple de génération: pwsh -c "[guid]::NewGuid().ToString('N').ToUpper()"
VLLM_API_KEY_MICRO=YOUR_MICRO_API_KEY_HERE_32_CHARS
VLLM_API_KEY_MINI=YOUR_MINI_API_KEY_HERE_32_CHARS
VLLM_API_KEY_MEDIUM=YOUR_MEDIUM_API_KEY_HERE_32_CHARS

# ============================================
# PORTS SERVICES
# ============================================
# Ports d'écoute pour chaque service (défaut si non spécifié dans docker-compose)
VLLM_PORT_MICRO=5000
VLLM_PORT_MINI=5001
VLLM_PORT_MEDIUM=5002

# ============================================
# URLS EXTERNES (OPTIONNEL)
# ============================================
# URLs publiques des APIs si exposées via reverse proxy/load balancer
VLLM_URL_MICRO=https://api.micro.your-domain.com/
VLLM_URL_MINI=https://api.mini.your-domain.com/
VLLM_URL_MEDIUM=https://api.medium.your-domain.com/

# ============================================
# CONFIGURATION GPU
# ============================================
# Numéros des GPUs à utiliser pour chaque service (séparés par virgules)
# Vérifier avec: nvidia-smi
CUDA_VISIBLE_DEVICES_MICRO=2
CUDA_VISIBLE_DEVICES_MINI=2
CUDA_VISIBLE_DEVICES_MEDIUM=0,1

# Utilisation mémoire GPU (0.0 à 1.0)
# ATTENTION: micro + mini partagent GPU 2, total ne doit pas dépasser ~0.90
GPU_MEMORY_UTILIZATION_MICRO=0.38
GPU_MEMORY_UTILIZATION_MINI=0.48
GPU_MEMORY_UTILIZATION_MEDIUM=0.88

# ============================================
# MODÈLES PAR DÉFAUT
# ============================================
# Chemins ou identifiants Hugging Face des modèles à charger
# micro: Qwen3-4B-Thinking (compact reasoning model, GPU 2 shared)
# Source: https://huggingface.co/cyankiwi/Qwen3-4B-Thinking-2507-AWQ-4bit
VLLM_MODEL_MICRO=cyankiwi/Qwen3-4B-Thinking-2507-AWQ-4bit

# mini/small: Qwen3-VL-8B-Thinking (vision + reasoning, GPU 2 shared)
# Source: https://huggingface.co/cyankiwi/Qwen3-VL-8B-Thinking-AWQ-4bit
VLLM_MODEL_MINI=cyankiwi/Qwen3-VL-8B-Thinking-AWQ-4bit

# medium: Qwen3-32B-AWQ (legacy, replaced by GLM-4.7-Flash)
VLLM_MODEL_MEDIUM=Qwen/Qwen3-32B-AWQ

# GLM-4.7-Flash (31B MoE, 3B active) - AWQ 4-bit from HuggingFace
# Source: https://huggingface.co/QuantTrio/GLM-4.7-Flash-AWQ
# Replaces Qwen3-Coder-Next (PP=3 bottleneck limited to 5-6 tok/s)
# TP=2 on GPUs 0,1 (faster PCIe), frees GPU 2 for other services
VLLM_MODEL_GLM=QuantTrio/GLM-4.7-Flash-AWQ
VLLM_PORT_GLM=5002
CUDA_VISIBLE_DEVICES_GLM=0,1

# Qwen3-Coder-Next (80B MoE) - ARCHIVED, use GLM-4.7-Flash instead
# PP=3 bottleneck: only 5-6 tok/s due to pipeline bubbles
# VLLM_MODEL_CODER=cyankiwi/Qwen3-Coder-Next-AWQ-4bit
# VLLM_PORT_CODER=5002
# CUDA_VISIBLE_DEVICES_CODER=0,1,2

# ============================================
# OPTIMISATIONS PERFORMANCE
# ============================================
# Nombre de threads OpenMP (1 recommandé pour éviter la contention)
OMP_NUM_THREADS=1

# Fuseau horaire
TZ=Europe/Paris

# Cache Hugging Face (optionnel, chemin personnalisé)
# Par défaut: ~/.cache/huggingface/hub
# HF_CACHE_PATH=/path/to/custom/cache

# ============================================
# PARAMÈTRES AVANCÉS (OPTIONNEL)
# ============================================
# Dtype par défaut pour les calculs
# DTYPE_MICRO=half
# DTYPE_MINI=half
# DTYPE_MEDIUM=half

# Longueur maximale de contexte (tokens)
# MAX_MODEL_LEN_MICRO=32768
# MAX_MODEL_LEN_MINI=65536
# MAX_MODEL_LEN_MEDIUM=131072

# Backend d'attention (FLASHINFER recommandé pour Qwen3)
# VLLM_ATTENTION_BACKEND=FLASHINFER

# Autoriser contextes plus longs que définition du modèle
# VLLM_ALLOW_LONG_MAX_MODEL_LEN=1