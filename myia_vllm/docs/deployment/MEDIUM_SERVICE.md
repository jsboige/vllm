# Guide de D√©ploiement - Service Medium (Qwen3-32B-AWQ)

**Date de cr√©ation**: 2025-10-16  
**Mission**: SDDD Mission 9 - Red√©ploiement Service Medium  
**Statut**: ‚úÖ PR√äT POUR D√âPLOIEMENT

---

## Table des Mati√®res

1. [Vue d'Ensemble](#vue-densemble)
2. [Pr√©requis](#pr√©requis)
3. [Validation de la Configuration](#validation-de-la-configuration)
4. [Proc√©dure de D√©ploiement](#proc√©dure-de-d√©ploiement)
5. [Monitoring et Surveillance](#monitoring-et-surveillance)
6. [Validation Post-D√©ploiement](#validation-post-d√©ploiement)
7. [Troubleshooting](#troubleshooting)

---

## Vue d'Ensemble

### Objectif

D√©ployer le service vLLM Medium configur√© pour le mod√®le **Qwen3-32B-AWQ** avec les param√®tres optimaux valid√©s lors de la Mission 9 SDDD.

### Caract√©ristiques du Service

| Param√®tre | Valeur | Notes |
|-----------|--------|-------|
| **Mod√®le** | Qwen/Qwen3-32B-AWQ | Quantization AWQ Marlin |
| **GPUs** | 2 (CUDA 0,1) | Tensor Parallel Size 2 |
| **Context Max** | 131072 tokens (128k) | ROPE scaling YARN 4.0 |
| **Memory Util** | 0.95 (95%) | Optimal pour production |
| **Port** | 5002 | Configurable via .env |
| **Container** | myia-vllm-medium-qwen3 | Docker Compose |
| **Image** | vllm/vllm-openai:latest | Image officielle |

### Temps de D√©ploiement Estim√©

- **D√©marrage conteneur** : 30-60 secondes
- **Chargement mod√®le** : 3-5 minutes
- **Health check** : Jusqu'√† 5 minutes (start_period)
- **TOTAL** : ~6-10 minutes

---

## Pr√©requis

### 1. Mat√©riel Requis

```yaml
GPUs: 2x NVIDIA (CUDA compatible)
VRAM: Minimum 24GB par GPU (recommand√©: 40GB+)
RAM: Minimum 64GB syst√®me
Disque: 100GB+ espace libre (mod√®le + cache)
```

### 2. Logiciels Requis

```bash
# V√©rifier Docker
docker --version
# Minimum: Docker 20.10+

# V√©rifier Docker Compose
docker compose version
# Minimum: Docker Compose 2.0+

# V√©rifier NVIDIA Container Toolkit
nvidia-smi
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

### 3. Configuration Environnement

**Fichier `.env` requis** : `myia_vllm/.env`

Variables critiques √† v√©rifier :
```bash
# HuggingFace Token (CRITIQUE)
HUGGING_FACE_HUB_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# API Key
VLLM_API_KEY_MEDIUM=your-secure-api-key-here

# GPUs
CUDA_VISIBLE_DEVICES_MEDIUM=0,1

# Port
VLLM_PORT_MEDIUM=5002

# Optimisations
GPU_MEMORY_UTILIZATION_MEDIUM=0.95
```

**üìö Documentation** : Voir [`ENV_CONFIGURATION.md`](../setup/ENV_CONFIGURATION.md) pour guide complet

### 4. Validation Pr√©-D√©ploiement

```powershell
# V√©rifier que le fichier .env existe et contient le token
Test-Path "myia_vllm/.env"

# V√©rifier que le profil medium existe
Test-Path "myia_vllm/configs/docker/profiles/medium.yml"

# V√©rifier disponibilit√© GPUs
nvidia-smi --query-gpu=index,name,memory.total,memory.free --format=csv
```

---

## Validation de la Configuration

### Configuration Valid√©e ‚úÖ

La configuration actuelle dans `configs/docker/profiles/medium.yml` a √©t√© valid√©e comme **100% optimale** :

- ‚úÖ **14/14 param√®tres** conformes aux meilleures pratiques
- ‚úÖ **Score 100%** selon checklist vLLM
- ‚úÖ **Score 100%** selon recommandations Qwen3
- ‚úÖ **Context 128k** maximis√©
- ‚úÖ **GPU memory 95%** optimis√©

**üìö Rapport d√©taill√©** : [`MEDIUM_SERVICE_PARAMETERS.md`](../docker/MEDIUM_SERVICE_PARAMETERS.md)

### Param√®tres Critiques Confirm√©s

```yaml
--model Qwen/Qwen3-32B-AWQ
--max-model-len 131072                    # ‚úÖ 128k tokens
--tensor-parallel-size 2                  # ‚úÖ 2 GPUs requis
--gpu-memory-utilization 0.95             # ‚úÖ 95% optimal
--quantization awq_marlin                 # ‚úÖ Meilleure perf
--kv_cache_dtype fp8                      # ‚úÖ √âconomie m√©moire
--enable-auto-tool-choice                 # ‚úÖ Tool calling
--tool-call-parser hermes                 # ‚úÖ Compatible
--reasoning-parser qwen3                  # ‚úÖ Natif Qwen3
--distributed-executor-backend=mp         # ‚úÖ Multiprocessing
--rope_scaling '{"rope_type":"yarn","factor":4.0,...}'  # ‚úÖ Extension contexte
```

---

## Proc√©dure de D√©ploiement

### M√©thode 1 : D√©ploiement avec Monitoring Int√©gr√© (RECOMMAND√â)

```powershell
# Script complet avec monitoring automatique
pwsh -c "./myia_vllm/scripts/deploy_medium_monitored.ps1"
```

**Avantages** :
- ‚úÖ Monitoring automatique toutes les 10s
- ‚úÖ D√©tection erreurs en temps r√©el
- ‚úÖ Timeout 10 minutes configurable
- ‚úÖ Logs d√©taill√©s sauvegard√©s

### M√©thode 2 : D√©ploiement Manuel

#### √âtape 1 : Arr√™t du service existant (si applicable)

```bash
docker compose \
  -f myia_vllm/configs/docker/docker-compose.yml \
  -f myia_vllm/configs/docker/profiles/medium.yml \
  down --remove-orphans
```

#### √âtape 2 : V√©rification de l'environnement

```bash
# V√©rifier que les GPUs sont libres
nvidia-smi

# V√©rifier l'espace disque
df -h | grep vllm

# V√©rifier que le port 5002 est libre
netstat -tuln | grep 5002
```

#### √âtape 3 : D√©ploiement

```bash
docker compose \
  -f myia_vllm/configs/docker/docker-compose.yml \
  -f myia_vllm/configs/docker/profiles/medium.yml \
  up -d --build --force-recreate
```

**Options expliqu√©es** :
- `-d` : Mode d√©tach√© (background)
- `--build` : Rebuild si changements Dockerfile
- `--force-recreate` : Recr√©er conteneurs existants

#### √âtape 4 : V√©rification imm√©diate

```bash
# V√©rifier que le conteneur d√©marre
docker ps -a | grep myia-vllm-medium-qwen3

# Suivre les logs en temps r√©el
docker logs -f myia-vllm-medium-qwen3
```

---

## Monitoring et Surveillance

### Script de Monitoring Automatique

**Localisation** : `myia_vllm/scripts/monitor_medium.ps1`

```powershell
# Monitoring avec intervalles personnalis√©s
pwsh -c "./myia_vllm/scripts/monitor_medium.ps1 -IntervalSeconds 10 -TimeoutMinutes 10"
```

**Fonctionnalit√©s** :
- ‚úÖ Surveillance status conteneur toutes les 10s
- ‚úÖ Extraction derniers logs (20 lignes)
- ‚úÖ D√©tection erreurs critiques (ERROR, FATAL, OOM)
- ‚úÖ Confirmation √©tat healthy
- ‚úÖ Timeout configurable (d√©faut 10 min)

### Surveillance Manuelle

#### V√©rifier le Status

```bash
# Status conteneur
docker ps --filter "name=myia-vllm-medium-qwen3"

# Status d√©taill√©
docker inspect myia-vllm-medium-qwen3 | grep -A 10 Health
```

#### Logs en Temps R√©el

```bash
# Logs complets
docker logs -f myia-vllm-medium-qwen3

# Filtrer erreurs
docker logs myia-vllm-medium-qwen3 2>&1 | grep -i error

# Derni√®res 50 lignes
docker logs myia-vllm-medium-qwen3 --tail 50
```

#### M√©triques GPU

```bash
# Utilisation GPU en temps r√©el
watch -n 2 nvidia-smi

# GPU du conteneur sp√©cifique
docker exec myia-vllm-medium-qwen3 nvidia-smi
```

### Logs Critiques √† Surveiller

**Phase 1 : D√©marrage (0-60s)**
```
‚úÖ "Starting vLLM engine"
‚úÖ "Initializing Qwen/Qwen3-32B-AWQ"
‚ö†Ô∏è Erreurs potentielles : "CUDA out of memory", "Failed to load model"
```

**Phase 2 : Chargement Mod√®le (60-300s)**
```
‚úÖ "Loading model weights"
‚úÖ "Loading safetensors checkpoint"
‚úÖ "Model loaded successfully"
‚ö†Ô∏è Erreurs potentielles : "Connection timeout", "Download failed"
```

**Phase 3 : Initialisation Engine (300-360s)**
```
‚úÖ "Initializing KV cache"
‚úÖ "Starting scheduler"
‚úÖ "Server is ready"
‚úÖ "Application startup complete"
‚ö†Ô∏è Erreurs potentielles : "Failed to initialize", "Scheduler error"
```

**Phase 4 : Healthy (360s+)**
```
‚úÖ Health check: "status: (healthy)"
‚úÖ "Uvicorn running on http://0.0.0.0:8000"
```

---

## Validation Post-D√©ploiement

### 1. Test Health Endpoint

```bash
# Test endpoint /health
curl -f http://localhost:5002/health

# R√©ponse attendue
# HTTP 200 OK
# {"status": "ok"}
```

### 2. Test Models Endpoint

```bash
# Lister mod√®les disponibles
curl http://localhost:5002/v1/models

# R√©ponse attendue
# {
#   "object": "list",
#   "data": [{
#     "id": "Qwen/Qwen3-32B-AWQ",
#     "object": "model",
#     ...
#   }]
# }
```

### 3. Test G√©n√©ration Simple

```bash
curl -X POST http://localhost:5002/v1/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer ${VLLM_API_KEY_MEDIUM}" \
  -d '{
    "model": "Qwen/Qwen3-32B-AWQ",
    "prompt": "Hello, how are you?",
    "max_tokens": 50
  }'
```

### 4. Test Tool Calling

```python
# Utiliser le script de test existant
python myia_vllm/tests/scripts/tests/test_qwen3_tool_calling.py \
  --endpoint http://localhost:5002/v1/chat/completions \
  --api-key ${VLLM_API_KEY_MEDIUM}
```

**üìö Script complet** : [`test_qwen3_tool_calling.py`](../../tests/scripts/tests/test_qwen3_tool_calling.py)

### 5. Test Reasoning

```python
# Utiliser le script de test existant
python myia_vllm/tests/scripts/tests/test_reasoning.py \
  --endpoint http://localhost:5002/v1/chat/completions \
  --api-key ${VLLM_API_KEY_MEDIUM}
```

**üìö Script complet** : [`test_reasoning.py`](../../tests/scripts/tests/test_reasoning.py)

### Checklist de Validation

- [ ] ‚úÖ Conteneur status: `Up` et `(healthy)`
- [ ] ‚úÖ Endpoint `/health` r√©pond HTTP 200
- [ ] ‚úÖ Endpoint `/v1/models` liste Qwen3-32B-AWQ
- [ ] ‚úÖ GPUs utilis√©s : CUDA 0 et 1 (via nvidia-smi)
- [ ] ‚úÖ Memory utilization : ~95% sur les 2 GPUs
- [ ] ‚úÖ Test g√©n√©ration simple fonctionne
- [ ] ‚úÖ Test tool calling r√©ussit
- [ ] ‚úÖ Test reasoning r√©ussit
- [ ] ‚úÖ Aucune erreur CUDA dans les logs
- [ ] ‚úÖ Aucun OOM event dans les logs

---

## Troubleshooting

### Probl√®me 1 : Conteneur ne d√©marre pas

**Sympt√¥mes** :
```bash
docker ps -a  # Status: Exited (1)
```

**Diagnostic** :
```bash
# V√©rifier les logs d'erreur
docker logs myia-vllm-medium-qwen3 2>&1 | tail -50
```

**Solutions courantes** :

1. **Token HuggingFace manquant/invalide**
   ```bash
   # V√©rifier dans .env
   grep HUGGING_FACE_HUB_TOKEN myia_vllm/.env
   
   # Tester le token
   curl -H "Authorization: Bearer ${HUGGING_FACE_HUB_TOKEN}" \
     https://huggingface.co/api/whoami
   ```

2. **GPUs non disponibles**
   ```bash

### Probl√®me 3 : GPUs Non D√©tect√©s ("Failed to infer device type")

**Sympt√¥mes** :
```bash
docker logs myia-vllm-medium-qwen3
# RuntimeError: Failed to infer device type
# ImportError: libcuda.so.1: cannot open shared object file
# INFO: No platform detected, vLLM is running on UnspecifiedPlatform
```

**Diagnostic** :

1. **V√©rifier runtime NVIDIA dans Docker** :
   ```bash
   docker info | grep -i nvidia
   # Doit afficher: Runtimes: nvidia runc
   ```

2. **Tester GPU dans conteneur basique** :
   ```bash
   docker run --rm --runtime=nvidia nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
   # Doit lister les 3x RTX 4090
   ```

3. **V√©rifier configuration Docker Compose** :
   ```bash
   # La syntaxe deploy.resources.reservations est pour Docker SWARM
   # Elle ne fonctionne PAS avec Docker Compose standalone
   ```

**Solution** :

Utiliser la syntaxe Docker Compose correcte dans [`medium.yml`](../../configs/docker/profiles/medium.yml) :

```yaml
services:
  vllm-medium-qwen3:
    runtime: nvidia              # ‚úÖ Runtime NVIDIA explicite
    ipc: host                     # ‚úÖ IPC pour communication inter-GPU
    shm_size: '16gb'             # ‚úÖ M√©moire partag√©e pour vLLM
    environment:
      - NVIDIA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES_MEDIUM:-0,1}
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
```

**Validation du Fix** :
```powershell
# D√©ploiement avec script officiel
pwsh -File scripts/deploy_medium_monitored.ps1

# Le monitoring doit afficher :
# ‚úÖ Automatically detected platform cuda
# ‚úÖ GPU KV cache size: 195,312 tokens
# ‚úÖ Available KV cache memory: 11.92 GiB
# ‚úÖ Status: healthy
```

**R√©f√©rences** :
- Docker Compose GPU : [`runtime: nvidia`](https://docs.docker.com/compose/gpu-support/)
- vLLM Best Practices : `ipc: host` + `shm_size` requis
- Fix appliqu√© : 2025-10-16

   # V√©rifier NVIDIA runtime
   docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
   
   # V√©rifier CUDA_VISIBLE_DEVICES
   echo $CUDA_VISIBLE_DEVICES_MEDIUM
   ```

3. **Port d√©j√† utilis√©**
   ```bash
   # V√©rifier port 5002
   netstat -tuln | grep 5002
   
   # Changer le port dans .env si n√©cessaire
   VLLM_PORT_MEDIUM=5003
   ```

### Probl√®me 2 : CUDA Out of Memory (OOM)

**Sympt√¥mes** :
```
RuntimeError: CUDA out of memory
```

**Solutions** :

1. **V√©rifier m√©moire GPU disponible**
   ```bash
   nvidia-smi --query-gpu=memory.free,memory.total --format=csv
   ```

2. **R√©duire gpu-memory-utilization (temporaire)**
   ```yaml
   # Dans .env
   GPU_MEMORY_UTILIZATION_MEDIUM=0.90  # Passer de 0.95 √† 0.90
   ```

3. **V√©rifier qu'aucun autre processus n'utilise les GPUs**
   ```bash
   nvidia-smi | grep -A 3 Processes
   
   # Lib√©rer si n√©cessaire
   kill -9 <PID>
   ```

### Probl√®me 3 : Mod√®le ne se charge pas

**Sympt√¥mes** :
```
Failed to load model weights
Connection timeout
```

**Solutions** :

1. **V√©rifier cache HuggingFace**
   ```bash
   # Localisation cache (selon OS)
   # Linux/WSL: ~/.cache/huggingface/hub
   # Windows: %USERPROFILE%\.cache\huggingface\hub
   
   # V√©rifier si mod√®le t√©l√©charg√©
   ls -lh ~/.cache/huggingface/hub/ | grep Qwen3-32B-AWQ
   ```

2. **Forcer re-t√©l√©chargement**
   ```bash
   # Supprimer cache mod√®le
   rm -rf ~/.cache/huggingface/hub/models--Qwen--Qwen3-32B-AWQ
   
   # Red√©marrer conteneur
   docker compose ... up -d --force-recreate
   ```

3. **V√©rifier connexion HuggingFace**
   ```bash
   # Test connexion
   curl -I https://huggingface.co/Qwen/Qwen3-32B-AWQ
   ```

### Probl√®me 4 : Health Check ne passe jamais

**Sympt√¥mes** :
```bash
docker ps  # Status: (unhealthy)
```

**Solutions** :

1. **V√©rifier que le mod√®le charge**
   ```bash
   # Suivre logs en temps r√©el
   docker logs -f myia-vllm-medium-qwen3
   
   # Chercher "Model loaded successfully"
   ```

2. **Augmenter start_period si chargement lent**
   ```yaml
   # Dans profiles/medium.yml
   healthcheck:
     start_period: 600s  # 10 min au lieu de 5 min
   ```

3. **Tester manuellement le endpoint**
   ```bash
   # Depuis l'h√¥te
   curl -f http://localhost:5002/health
   
   # Depuis le conteneur
   docker exec myia-vllm-medium-qwen3 curl -f http://localhost:8000/health
   ```

### Probl√®me 5 : Performance d√©grad√©e

**Sympt√¥mes** :
- Latence √©lev√©e (>5s pour premi√®re token)
- Throughput faible (<50 tokens/sec)

**Solutions** :

1. **V√©rifier utilisation GPU**
   ```bash
   watch -n 1 nvidia-smi
   # GPU-Util devrait √™tre ~90-100% pendant inf√©rence
   ```

2. **V√©rifier contexte utilis√©**
   ```bash
   # Dans les logs, chercher
   docker logs myia-vllm-medium-qwen3 | grep "context_len"
   ```

3. **V√©rifier param√®tres quantization**
   ```bash
   # Confirmer awq_marlin actif
   docker logs myia-vllm-medium-qwen3 | grep quantization
   # Devrait voir: "Using quantization: awq_marlin"
   ```

---

## Commandes de R√©f√©rence Rapide

### D√©ploiement
```bash
# Avec monitoring
pwsh -c "./myia_vllm/scripts/deploy_medium_monitored.ps1"

# Manuel
docker compose -f myia_vllm/configs/docker/docker-compose.yml \
  -f myia_vllm/configs/docker/profiles/medium.yml up -d --force-recreate
```

### Monitoring
```bash
# Logs temps r√©el
docker logs -f myia-vllm-medium-qwen3

# Status
docker ps --filter "name=myia-vllm-medium-qwen3"

# GPU
nvidia-smi
```

### Tests
```bash
# Health
curl -f http://localhost:5002/health

# Models
curl http://localhost:5002/v1/models

# Tool calling
python myia_vllm/tests/scripts/tests/test_qwen3_tool_calling.py
```

### Arr√™t
```bash
# Arr√™t propre
docker compose -f myia_vllm/configs/docker/docker-compose.yml \
  -f myia_vllm/configs/docker/profiles/medium.yml down --remove-orphans

# Force stop
docker stop myia-vllm-medium-qwen3
docker rm myia-vllm-medium-qwen3
```

---

## Prochaines √âtapes

Apr√®s d√©ploiement r√©ussi :

1. **Benchmarking** : Utiliser `qwen3_benchmark/` pour tests de charge
2. **Monitoring Production** : Int√©grer Prometheus/Grafana si n√©cessaire
3. **Optimisation** : Ajuster param√®tres selon m√©triques r√©elles
4. **Documentation** : Mettre √† jour ce guide avec retours terrain

---

## R√©f√©rences

### Documentation Interne

- **Configuration valid√©e** : [`MEDIUM_SERVICE_PARAMETERS.md`](../docker/MEDIUM_SERVICE_PARAMETERS.md)
- **Architecture Docker** : [`ARCHITECTURE.md`](../docker/ARCHITECTURE.md)
- **Configuration .env** : [`ENV_CONFIGURATION.md`](../setup/ENV_CONFIGURATION.md)

### Scripts

- **Monitoring** : `myia_vllm/scripts/monitor_medium.ps1`
- **D√©ploiement** : `myia_vllm/scripts/deploy_medium_monitored.ps1`
- **Tests** : `myia_vllm/tests/scripts/tests/test_qwen3_tool_calling.py`

### Documentation Externe

- **vLLM Official** : https://docs.vllm.ai/
- **Qwen3 Model Card** : https://huggingface.co/Qwen/Qwen3-32B-AWQ
- **Docker Compose** : https://docs.docker.com/compose/

---

**Derni√®re mise √† jour** : 2025-10-16  
**Mission** : SDDD Mission 9 - Red√©ploiement Service Medium  
**Prochaine r√©vision** : Apr√®s premier d√©ploiement production