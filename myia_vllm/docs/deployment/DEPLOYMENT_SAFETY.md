# üõ°Ô∏è Guide de S√©curit√© D√©ploiement - Service Medium vLLM

**Date de cr√©ation** : 2025-10-16  
**Mission** : SDDD Mission 9 - Audit Critique Pr√©-D√©ploiement  
**Statut** : üö® LECTURE OBLIGATOIRE AVANT TOUT D√âPLOIEMENT

---

## ‚ö†Ô∏è ALERTE CRITIQUE : CONFUSION ARCHITECTURALE D√âTECT√âE

### üîç Probl√®me Identifi√©

Le projet contient **DEUX architectures Docker parall√®les** cr√©ant une **confusion dangereuse** :

1. **Architecture Autonome** (racine du projet)
   - `docker-compose-qwen3-medium.yml`
   - `docker-compose-qwen3-micro.yml`
   - `docker-compose-qwen3-mini.yml`

2. **Architecture Modulaire** (configs/)
   - `configs/docker/profiles/medium.yml`
   - Fichier base `configs/docker/docker-compose.yml` : **‚ùå N'EXISTE PAS**

### üéØ Configuration Utilis√©e par le Script de D√©ploiement

Le script `scripts/deploy_medium_monitored.ps1` (ligne 43) utilise :
```powershell
$composeProfile = "configs\docker\profiles\medium.yml"
```

**‚ö†Ô∏è PROBL√àME** : Ce fichier utilise l'image Docker `latest` (instable) au lieu de `v0.9.2` (stable).

---

## üìä ANALYSE COMPARATIVE CRITIQUE DES CONFIGURATIONS

### Fichier 1 : `docker-compose-qwen3-medium.yml` (racine)

| Param√®tre | Valeur | √âvaluation |
|-----------|--------|------------|
| Image Docker | `v0.9.2` | ‚úÖ **Version stable** |
| Context Max | 70000 tokens | ‚ö†Ô∏è **Sous-optimal** (128k possible) |
| Quantization | Non sp√©cifi√©e | ‚ö†Ô∏è **Manque explicite** |
| Executor Backend | ray (d√©faut) | ‚ö†Ô∏è **Non optimal pour TP** |
| Healthcheck Start | 30s | ‚ùå **Trop court** (mod√®le 32B) |
| Container Name | Non d√©fini | ‚ö†Ô∏è **Monitoring difficile** |
| Swap Space | Non d√©fini | ‚ö†Ô∏è **Risque OOM** |

**Score Global** : üü° **5/10** - Stable mais sous-optimal

### Fichier 2 : `configs/docker/profiles/medium.yml` (configs/)

| Param√®tre | Valeur | √âvaluation |
|-----------|--------|------------|
| Image Docker | `latest` | ‚ùå **INSTABLE - BLOQUANT** |
| Context Max | 131072 tokens (128k) | ‚úÖ **Optimal** |
| Quantization | `awq_marlin` | ‚úÖ **Explicite et optimal** |
| Executor Backend | `mp` | ‚úÖ **Optimal pour TP** |
| Healthcheck Start | 300s (5min) | ‚úÖ **R√©aliste pour 32B** |
| Container Name | `myia-vllm-medium-qwen3` | ‚úÖ **Monitoring facile** |
| Swap Space | 16GB | ‚úÖ **Protection OOM** |

**Score Global** : üî¥ **3/10** - Param√®tres optimaux MAIS version instable

---

## üéØ RECOMMANDATION OFFICIELLE

### ‚úÖ Configuration √† Utiliser : AUCUNE DES DEUX EN L'√âTAT

**Les deux fichiers ont des d√©fauts critiques.** Avant tout d√©ploiement :

### Option A : Corriger `configs/docker/profiles/medium.yml` (RECOMMAND√â)

**Modification requise** :
```yaml
# AVANT (ligne 3)
image: vllm/vllm-openai:latest

# APR√àS
image: vllm/vllm-openai:v0.9.2
```

**Avantages** :
- ‚úÖ Param√®tres optimaux (131k context, awq_marlin, mp backend)
- ‚úÖ Architecture modulaire document√©e
- ‚úÖ Utilis√© par le script de d√©ploiement officiel

**Inconv√©nient** :
- ‚ö†Ô∏è N√©cessite modification manuelle avant d√©ploiement

### Option B : Utiliser `docker-compose-qwen3-medium.yml` avec am√©liorations

**Modifications requises** :
1. Augmenter `max-model-len` : 70000 ‚Üí 131072
2. Ajouter `--quantization awq_marlin`
3. Ajouter `--distributed-executor-backend=mp`
4. Augmenter healthcheck `start_period` : 30s ‚Üí 300s
5. Ajouter `container_name: myia-vllm-medium-qwen3`

**Avantages** :
- ‚úÖ Version stable v0.9.2
- ‚úÖ Fichier autonome (pas de d√©pendance)

**Inconv√©nients** :
- ‚ö†Ô∏è Nombreuses modifications requises
- ‚ö†Ô∏è N√©cessite modifier le script de d√©ploiement (ligne 43)

---

## üö® CHECKLIST DE S√âCURIT√â PR√â-D√âPLOIEMENT

### 1Ô∏è‚É£ Validation Configuration Docker

- [ ] **Image Docker** : V√©rifier version `v0.9.2` (PAS `latest`)
  ```bash
  grep "image:" configs/docker/profiles/medium.yml
  # Doit afficher: image: vllm/vllm-openai:v0.9.2
  ```

- [ ] **Syntaxe YAML** : Valider avec `docker compose config`
  ```bash
  docker compose -f configs/docker/profiles/medium.yml config
  # Ne doit retourner AUCUNE erreur
  ```

- [ ] **Param√®tres critiques** : V√©rifier pr√©sence
  ```yaml
  --max-model-len 131072        ‚úÖ Requis
  --quantization awq_marlin     ‚úÖ Requis
  --distributed-executor-backend=mp  ‚úÖ Requis
  --tensor-parallel-size 2      ‚úÖ Requis
  ```

### 2Ô∏è‚É£ √âtat Syst√®me Pr√©-D√©ploiement

- [ ] **Backup √©tat actuel** : Documenter conteneurs running
  ```bash
  docker ps --format "{{.ID}}\t{{.Names}}\t{{.Status}}" > backup_containers_$(date +%Y%m%d_%H%M%S).txt
  docker inspect myia-vllm-medium-qwen3 > backup_config_$(date +%Y%m%d_%H%M%S).json 2>/dev/null || echo "Pas de conteneur existant"
  ```

- [ ] **GPUs disponibles** : V√©rifier allocation
  ```bash
  nvidia-smi --query-gpu=index,name,utilization.gpu,memory.used,memory.total --format=csv
  # GPU 0,1 doivent √™tre libres ou <50% utilisation
  ```

- [ ] **Espace disque** : Minimum 100GB libre
  ```bash
  df -h ~/ | grep -E "Avail|Available"
  ```

- [ ] **Port 5002 libre** : V√©rifier disponibilit√©
  ```bash
  netstat -tuln | grep 5002 || echo "Port libre"
  ```

### 3Ô∏è‚É£ Configuration Environnement (.env)

- [ ] **Fichier .env existe** : `myia_vllm/.env`
  
- [ ] **Token HuggingFace valide** : Commence par `hf_`
  ```bash
  grep "HUGGING_FACE_HUB_TOKEN=hf_" myia_vllm/.env
  ```

- [ ] **Variables critiques d√©finies** :
  ```bash
  # V√©rifier pr√©sence (sans afficher valeurs)
  grep -q "CUDA_VISIBLE_DEVICES_MEDIUM" myia_vllm/.env && echo "‚úÖ CUDA_VISIBLE_DEVICES_MEDIUM"
  grep -q "VLLM_PORT_MEDIUM" myia_vllm/.env && echo "‚úÖ VLLM_PORT_MEDIUM"
  grep -q "VLLM_API_KEY_MEDIUM" myia_vllm/.env && echo "‚úÖ VLLM_API_KEY_MEDIUM"
  ```

### 4Ô∏è‚É£ Plan de Rollback Pr√©par√©

- [ ] **Identifier configuration actuelle** (si applicable)
  ```bash
  docker inspect myia-vllm-medium-qwen3 --format='{{.Config.Image}}'
  ```

- [ ] **Sauvegarder logs actuels** (si conteneur existe)
  ```bash
  docker logs myia-vllm-medium-qwen3 > logs_backup_$(date +%Y%m%d_%H%M%S).txt 2>&1 || echo "Pas de logs"
  ```

- [ ] **Documenter commande de rollback**
  ```bash
  # Si probl√®me, revenir √† l'ancienne version :
  docker compose -f configs/docker/profiles/medium.yml down
  # Puis restaurer backup si n√©cessaire
  ```

---

## üõ°Ô∏è PROC√âDURE DE D√âPLOIEMENT S√âCURIS√â

### Phase 1 : Pr√©paration (10 minutes)

#### 1.1 Corriger la Configuration Docker

**Fichier** : `configs/docker/profiles/medium.yml`

**Modification ligne 3** :
```yaml
# AVANT
image: vllm/vllm-openai:latest

# APR√àS
image: vllm/vllm-openai:v0.9.2
```

**Commande de correction rapide** :
```powershell
# Windows PowerShell
(Get-Content myia_vllm/configs/docker/profiles/medium.yml) -replace 'image: vllm/vllm-openai:latest', 'image: vllm/vllm-openai:v0.9.2' | Set-Content myia_vllm/configs/docker/profiles/medium.yml
```

#### 1.2 Valider la Configuration

```bash
cd myia_vllm
docker compose -f configs/docker/profiles/medium.yml config
```

**R√©sultat attendu** : Aucune erreur, affichage de la config compl√®te

#### 1.3 V√©rifier le Fichier .env

```bash
# V√©rifier pr√©sence variables critiques (sans afficher valeurs)
cat .env | grep -E "HUGGING_FACE_HUB_TOKEN|CUDA_VISIBLE_DEVICES_MEDIUM|VLLM_PORT_MEDIUM|VLLM_API_KEY_MEDIUM" | wc -l
# Doit retourner: 4
```

### Phase 2 : D√©ploiement S√©curis√© (15-20 minutes)

#### 2.1 Mode Dry-Run (Simulation)

**‚ö†Ô∏è Actuellement NON disponible dans le script**

Alternative manuelle :
```bash
# V√©rifier ce qui serait cr√©√© sans l'ex√©cuter
docker compose -f configs/docker/profiles/medium.yml config
```

#### 2.2 Backup √âtat Actuel

```bash
# Cr√©er r√©pertoire backups
mkdir -p backups/$(date +%Y%m%d_%H%M%S)
cd backups/$(date +%Y%m%d_%H%M%S)

# Sauvegarder √©tat
docker ps > containers_before.txt
docker logs myia-vllm-medium-qwen3 > logs_before.txt 2>&1 || echo "Pas de logs"
docker inspect myia-vllm-medium-qwen3 > config_before.json 2>&1 || echo "Pas de config"
nvidia-smi > gpu_before.txt

cd ../..
```

#### 2.3 D√©ploiement avec Monitoring

```powershell
# Option 1 : Utiliser le script officiel (apr√®s correction image)
pwsh -c "./scripts/deploy_medium_monitored.ps1"

# Option 2 : D√©ploiement manuel avec monitoring s√©par√©
docker compose -f configs/docker/profiles/medium.yml up -d --build --force-recreate
pwsh -c "./scripts/monitor_medium.ps1"
```

### Phase 3 : Validation Post-D√©ploiement (5-10 minutes)

#### 3.1 V√©rifier D√©marrage Conteneur

```bash
docker ps --filter "name=myia-vllm-medium-qwen3"
# Doit montrer le conteneur en status "Up"
```

#### 3.2 Surveiller Logs de D√©marrage

```bash
docker logs -f myia-vllm-medium-qwen3 --tail 100
```

**Indicateurs de succ√®s** :
- ‚úÖ `Uvicorn running on http://0.0.0.0:5002`
- ‚úÖ `Loaded model Qwen/Qwen3-32B-AWQ`
- ‚úÖ `GPU memory utilization: 95%`

**Indicateurs d'√©chec** :
- ‚ùå `CUDA out of memory`
- ‚ùå `Model not found`
- ‚ùå `Token authentication failed`

#### 3.3 Tests de Sant√©

```bash
# Test 1 : Health endpoint (sans auth)
curl -f http://localhost:5002/health
# Attendu: {"status":"ok"} ou similar

# Test 2 : Models endpoint (avec auth si configur√©e)
curl -H "Authorization: Bearer ${VLLM_API_KEY_MEDIUM}" http://localhost:5002/v1/models
# Attendu: Liste des mod√®les charg√©s

# Test 3 : GPU Utilization
nvidia-smi --query-gpu=index,utilization.gpu,memory.used --format=csv
# GPU 0,1 doivent montrer utilisation
```

---

## üîÑ PROC√âDURE DE ROLLBACK

### En Cas d'√âchec du D√©ploiement

#### 1. Arr√™t du Service Probl√©matique

```bash
docker compose -f configs/docker/profiles/medium.yml down --remove-orphans
```

#### 2. Diagnostic des Logs

```bash
# Sauvegarder logs d'√©chec
docker logs myia-vllm-medium-qwen3 > logs_failure_$(date +%Y%m%d_%H%M%S).txt 2>&1

# Analyser causes communes
grep -i "error\|failed\|exception" logs_failure_*.txt
```

#### 3. Restauration √âtat Pr√©c√©dent

Si un conteneur fonctionnait avant :

```bash
# Option A : Red√©marrer conteneur existant
docker start myia-vllm-medium-qwen3

# Option B : Recr√©er avec ancienne config
# (si backup de config disponible)
docker run --rm -v $(pwd)/backups/config_before.json:/config.json ...
```

#### 4. R√©initialisation Compl√®te

Si n√©cessaire revenir √† √©tat propre :

```bash
# Arr√™ter TOUS les conteneurs vLLM
docker ps -a --filter "name=vllm" --format "{{.ID}}" | xargs -r docker rm -f

# Nettoyer volumes orphelins
docker volume prune -f

# Nettoyer images inutilis√©es
docker image prune -a -f
```

---

## üö´ ACTIONS INTERDITES SANS VALIDATION UTILISATEUR

### ‚ùå NE JAMAIS EX√âCUTER SANS CONFIRMATION :

1. **`docker compose down` sur service en production**
   - Risque : Arr√™t imm√©diat du service
   - Alternative : Backup d'abord, puis arr√™t planifi√©

2. **`--force-recreate` sans backup**
   - Risque : Perte config actuelle
   - Alternative : Backup config + logs avant

3. **Modifications directes du .env en production**
   - Risque : Exposition secrets dans historique
   - Alternative : Utiliser `.env.local` non versionn√©

4. **Pull `latest` en production**
   - Risque : Breaking changes non test√©s
   - Alternative : Toujours sp√©cifier version (v0.9.2)

5. **Changement `CUDA_VISIBLE_DEVICES` sans test**
   - Risque : Mod√®le ne charge pas (tensor-parallel-size incompatible)
   - Alternative : V√©rifier GPU allocation avant

---

## üìã TROUBLESHOOTING GUIDE RAPIDE

### Probl√®me 1 : "Image tag 'latest' not stable"

**Cause** : Fichier `configs/docker/profiles/medium.yml` utilise `latest`

**Solution** :
```yaml
# Ligne 3 de configs/docker/profiles/medium.yml
image: vllm/vllm-openai:v0.9.2  # Changer latest ‚Üí v0.9.2
```

### Probl√®me 2 : "CUDA out of memory"

**Causes possibles** :
- GPU d√©j√† utilis√©s par autre processus
- `gpu-memory-utilization` trop √©lev√© (>0.95)
- Pas assez de VRAM pour mod√®le 32B

**Solutions** :
```bash
# 1. V√©rifier processus GPU
nvidia-smi

# 2. Lib√©rer GPUs
docker stop $(docker ps -q)  # ATTENTION: Arr√™te TOUS conteneurs

# 3. R√©duire utilisation m√©moire dans config
--gpu-memory-utilization 0.90  # Au lieu de 0.95
```

### Probl√®me 3 : "Token authentication failed"

**Cause** : Token HuggingFace manquant ou invalide

**Solution** :
```bash
# 1. V√©rifier token dans .env
grep HUGGING_FACE_HUB_TOKEN myia_vllm/.env

# 2. Tester token
curl -H "Authorization: Bearer hf_YOUR_TOKEN" https://huggingface.co/api/whoami-v2

# 3. Reg√©n√©rer token si besoin sur huggingface.co/settings/tokens
```

### Probl√®me 4 : "Healthcheck failing after deployment"

**Causes possibles** :
- Mod√®le encore en chargement (normal <5min)
- Port 5002 non accessible
- API Key requise mais non fournie

**Solutions** :
```bash
# 1. Attendre fin chargement
docker logs -f myia-vllm-medium-qwen3 | grep "Uvicorn running"

# 2. Tester port
curl http://localhost:5002/health

# 3. V√©rifier auth si configur√©e
curl -H "Authorization: Bearer ${VLLM_API_KEY_MEDIUM}" http://localhost:5002/health
```

---

## üìö R√âF√âRENCES

### Documentation Interne

- **Configuration valid√©e** : [`MEDIUM_SERVICE_PARAMETERS.md`](../docker/MEDIUM_SERVICE_PARAMETERS.md)
- **Architecture Docker** : [`ARCHITECTURE.md`](../docker/ARCHITECTURE.md)
- **Guide d√©ploiement** : [`MEDIUM_SERVICE.md`](./MEDIUM_SERVICE.md)
- **Configuration .env** : [`ENV_CONFIGURATION.md`](../setup/ENV_CONFIGURATION.md)

### Scripts Associ√©s

- **D√©ploiement avec monitoring** : `scripts/deploy_medium_monitored.ps1`
- **Monitoring seul** : `scripts/monitor_medium.ps1`

### Documentation Externe

- **vLLM Official Docs** : https://docs.vllm.ai/
- **Qwen3 Model Card** : https://huggingface.co/Qwen/Qwen3-32B-AWQ
- **Docker Compose Docs** : https://docs.docker.com/compose/

---

## ‚öñÔ∏è PRINCIPES DE S√âCURIT√â ADOPT√âS

1. **Principe de Version Stable** : Toujours sp√©cifier version explicite (`v0.9.2`), jamais `latest`

2. **Principe de Backup Avant Modification** : Toujours documenter √©tat actuel avant changement

3. **Principe de Validation Progressive** : 
   - Valider syntaxe ‚Üí Valider config ‚Üí Backup ‚Üí D√©ployer ‚Üí Monitorer

4. **Principe de Rollback Pr√©par√©** : Toujours avoir un plan B avant toute action

5. **Principe de Non-Interf√©rence** : Ne jamais modifier un syst√®me en production sans fen√™tre de maintenance

---

**Derni√®re mise √† jour** : 2025-10-16  
**Valid√© par** : SDDD Mission 9 - Audit Critique Pr√©-D√©ploiement  
**Prochaine r√©vision** : Apr√®s premier d√©ploiement s√©curis√© r√©ussi

---

## üîß Troubleshooting GPU Docker

### Probl√®me : "Failed to infer device type" / "libcuda.so.1: cannot open shared object file"

**Sympt√¥mes** :
```
RuntimeError: Failed to infer device type
ImportError: libcuda.so.1: cannot open shared object file: No such file or directory
INFO: No platform detected, vLLM is running on UnspecifiedPlatform
```

**Cause Racine** :
La syntaxe `deploy.resources.reservations.devices` est pour **Docker Swarm**, pas Docker Compose standalone. Le conteneur ne peut pas acc√©der aux GPUs NVIDIA.

**Solution** :

1. **V√©rifier le runtime NVIDIA** :
   ```bash
   docker info | grep -i nvidia
   # Doit afficher: Runtimes: nvidia runc
   
   docker run --rm --runtime=nvidia nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
   # Doit afficher les GPUs disponibles
   ```

2. **Corriger la configuration Docker Compose** :

   ‚ùå **INCORRECT (syntaxe Swarm)** :
   ```yaml
   deploy:
     resources:
       reservations:
         devices:
           - driver: nvidia
             capabilities: [gpu]
             device_ids: ['${CUDA_VISIBLE_DEVICES}']
   ```

   ‚úÖ **CORRECT (Docker Compose standalone)** :
   ```yaml
   runtime: nvidia
   ipc: host
   shm_size: '16gb'
   environment:
     - NVIDIA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-0,1}
     - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
   ```

3. **Tester la configuration** :
   ```bash
   # Test GPU dans conteneur
   docker compose -f medium.yml run --rm vllm-medium-qwen3 nvidia-smi
   
   # D√©ploiement complet
   pwsh -File scripts/deploy_medium_monitored.ps1
   ```

**R√©f√©rences** :
- vLLM documentation : Recommande `runtime: nvidia` + `ipc: host` + `shm_size`
- Docker Compose vs Docker Swarm : Syntaxes GPU diff√©rentes
- Fix appliqu√© le : 2025-10-16 (Mission Debug GPU)
