on # ðŸ“‹ PLAN DE TESTS - SERVICE MEDIUM QWEN3-32B-AWQ

**Service** : vLLM Medium (Qwen3-32B-AWQ)  
**Version vLLM** : v0.11.0  
**Status Document** : âš ï¸ **TESTS NON EXÃ‰CUTÃ‰S** - Documentation uniquement  
**Date CrÃ©ation** : 2025-10-16

---

## âš ï¸ AVERTISSEMENT IMPORTANT

**Ce document dÃ©crit les tests Ã  effectuer ULTÃ‰RIEUREMENT**. Les tests n'ont PAS Ã©tÃ© exÃ©cutÃ©s lors du dÃ©ploiement initial (MISSION 9 - PHASE 9).

Le service medium a Ã©tÃ© dÃ©ployÃ© avec succÃ¨s et est opÃ©rationnel, mais **aucun test fonctionnel n'a Ã©tÃ© effectuÃ©** au-delÃ  du health check basique.

---

## ðŸŽ¯ OBJECTIFS DES TESTS

### Tests de Base (PrioritÃ© CRITIQUE)
1. âœ… Health Check API (`/health`) - **EFFECTUÃ‰**
2. âœ… Liste ModÃ¨les (`/v1/models`) - **EFFECTUÃ‰**
3. ðŸ”„ Chat Completion Basique - **Ã€ FAIRE**
4. ðŸ”„ Text Completion Basique - **Ã€ FAIRE**

### Tests Fonctionnels AvancÃ©s (PrioritÃ© HAUTE)
5. ðŸ”„ Raisonnement Multi-Step (Chain-of-Thought)
6. ðŸ”„ Tool Calling avec Qwen3
7. ðŸ”„ Contexte Long (>64k tokens)
8. ðŸ”„ Streaming Responses

### Tests de Performance (PrioritÃ© MOYENNE)
9. ðŸ”„ Latence PremiÃ¨re RÃ©ponse (TTFT)
10. ðŸ”„ Throughput (tokens/sec)
11. ðŸ”„ Tests de Charge (concurrence)
12. ðŸ”„ StabilitÃ© Longue DurÃ©e

### Tests de Robustesse (PrioritÃ© BASSE)
13. ðŸ”„ Gestion Erreurs API
14. ðŸ”„ Limites Context Window
15. ðŸ”„ RÃ©sistance aux Crashes

---

## ðŸ“š TESTS DE BASE

### Test 1 : Health Check âœ… EFFECTUÃ‰

**Objectif** : VÃ©rifier que l'API rÃ©pond correctement

**Commande** :
```bash
curl http://localhost:5002/health
```

**RÃ©sultat Attendu** :
```
HTTP 200 OK
```

**RÃ©sultat Obtenu** :
âœ… HTTP 200 OK - Service opÃ©rationnel

---

### Test 2 : Liste ModÃ¨les âœ… EFFECTUÃ‰

**Objectif** : VÃ©rifier que le modÃ¨le est chargÃ© et disponible

**Commande** :
```bash
curl http://localhost:5002/v1/models
```

**RÃ©sultat Attendu** :
```json
{
  "object": "list",
  "data": [
    {
      "id": "Qwen2.5-32B-Instruct-AWQ",
      "object": "model",
      "created": ...,
      "owned_by": "vllm"
    }
  ]
}
```

**RÃ©sultat Obtenu** :
âœ… ModÃ¨le `Qwen2.5-32B-Instruct-AWQ` disponible

---

### Test 3 : Chat Completion Basique ðŸ”„ Ã€ FAIRE

**Objectif** : Tester la gÃ©nÃ©ration de texte via l'API chat

**Script Python** :
```python
# myia_vllm/tests/test_medium_chat_basic.py

import requests
import json

def test_chat_completion_basic():
    """Test basique de chat completion avec prompt simple"""
    
    url = "http://localhost:5002/v1/chat/completions"
    
    payload = {
        "model": "Qwen2.5-32B-Instruct-AWQ",
        "messages": [
            {
                "role": "user",
                "content": "Qu'est-ce que l'intelligence artificielle en une phrase ?"
            }
        ],
        "max_tokens": 100,
        "temperature": 0.7
    }
    
    response = requests.post(url, json=payload)
    
    assert response.status_code == 200, f"Expected 200, got {response.status_code}"
    
    data = response.json()
    assert "choices" in data, "Response missing 'choices' field"
    assert len(data["choices"]) > 0, "No choices returned"
    assert "message" in data["choices"][0], "No message in first choice"
    
    message = data["choices"][0]["message"]["content"]
    assert len(message) > 0, "Empty message returned"
    
    print("âœ… Test Chat Completion Basique RÃ‰USSI")
    print(f"RÃ©ponse: {message}")
    
    return data

if __name__ == "__main__":
    test_chat_completion_basic()
```

**Commande d'exÃ©cution** :
```bash
python myia_vllm/tests/test_medium_chat_basic.py
```

**CritÃ¨res de SuccÃ¨s** :
- âœ… HTTP 200
- âœ… RÃ©ponse contient `choices[0].message.content`
- âœ… Contenu non vide
- âœ… Temps de rÃ©ponse < 5 secondes

---

### Test 4 : Text Completion Basique ðŸ”„ Ã€ FAIRE

**Objectif** : Tester la gÃ©nÃ©ration via endpoint completions

**Script Python** :
```python
# myia_vllm/tests/test_medium_completion_basic.py

import requests

def test_text_completion_basic():
    """Test basique de text completion"""
    
    url = "http://localhost:5002/v1/completions"
    
    payload = {
        "model": "Qwen2.5-32B-Instruct-AWQ",
        "prompt": "L'intelligence artificielle est",
        "max_tokens": 50,
        "temperature": 0.7
    }
    
    response = requests.post(url, json=payload)
    
    assert response.status_code == 200
    
    data = response.json()
    assert "choices" in data
    assert len(data["choices"]) > 0
    
    text = data["choices"][0]["text"]
    assert len(text) > 0
    
    print("âœ… Test Text Completion Basique RÃ‰USSI")
    print(f"Texte gÃ©nÃ©rÃ©: {text}")
    
    return data

if __name__ == "__main__":
    test_text_completion_basic()
```

**CritÃ¨res de SuccÃ¨s** :
- âœ… HTTP 200
- âœ… GÃ©nÃ©ration cohÃ©rente
- âœ… Respecte max_tokens

---

## ðŸ§  TESTS FONCTIONNELS AVANCÃ‰S

### Test 5 : Raisonnement Multi-Step ðŸ”„ Ã€ FAIRE

**Objectif** : VÃ©rifier capacitÃ© de raisonnement Chain-of-Thought

**Base de rÃ©fÃ©rence** : [`myia_vllm/tests/scripts/tests/test_reasoning.py`](../../tests/scripts/tests/test_reasoning.py)

**Script Python** :
```python
# myia_vllm/tests/test_medium_reasoning.py

import requests

def test_reasoning_chain_of_thought():
    """Test raisonnement multi-Ã©tapes avec Chain-of-Thought"""
    
    url = "http://localhost:5002/v1/chat/completions"
    
    prompt = """RÃ©sous ce problÃ¨me Ã©tape par Ã©tape :

Un train part de Paris Ã  10h00 Ã  120 km/h.
Un autre train part de Lyon (450 km de Paris) Ã  10h30 Ã  150 km/h vers Paris.
Ã€ quelle heure se croisent-ils ?

Pense Ã©tape par Ã©tape :
1. Distance initiale
2. Vitesses relatives
3. Calcul du temps
4. Heure de croisement
"""
    
    payload = {
        "model": "Qwen2.5-32B-Instruct-AWQ",
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 500,
        "temperature": 0.1  # Basse tempÃ©rature pour prÃ©cision
    }
    
    response = requests.post(url, json=payload)
    
    assert response.status_code == 200
    
    data = response.json()
    answer = data["choices"][0]["message"]["content"]
    
    # VÃ©rifier que la rÃ©ponse contient les Ã©tapes
    assert "Ã©tape" in answer.lower() or "step" in answer.lower()
    
    print("âœ… Test Raisonnement Multi-Step RÃ‰USSI")
    print(f"\nRÃ©ponse:\n{answer}")
    
    return data

if __name__ == "__main__":
    test_reasoning_chain_of_thought()
```

**CritÃ¨res de SuccÃ¨s** :
- âœ… DÃ©composition en Ã©tapes
- âœ… Calculs corrects
- âœ… RÃ©ponse finale cohÃ©rente
- âœ… Format structurÃ©

---

### Test 6 : Tool Calling Qwen3 ðŸ”„ Ã€ FAIRE

**Objectif** : Tester appels de fonctions avec Qwen3

**Base de rÃ©fÃ©rence** : [`myia_vllm/tests/scripts/tests/test_qwen3_tool_calling.py`](../../tests/scripts/tests/test_qwen3_tool_calling.py)

**Script Python** :
```python
# myia_vllm/tests/test_medium_tool_calling.py

import requests
import json

def test_tool_calling_weather():
    """Test tool calling avec fonction mÃ©tÃ©o"""
    
    url = "http://localhost:5002/v1/chat/completions"
    
    # DÃ©finir la fonction disponible
    tools = [
        {
            "type": "function",
            "function": {
                "name": "get_weather",
                "description": "Obtenir la mÃ©tÃ©o actuelle pour une ville",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "city": {
                            "type": "string",
                            "description": "Nom de la ville"
                        },
                        "unit": {
                            "type": "string",
                            "enum": ["celsius", "fahrenheit"],
                            "description": "UnitÃ© de tempÃ©rature"
                        }
                    },
                    "required": ["city"]
                }
            }
        }
    ]
    
    payload = {
        "model": "Qwen2.5-32B-Instruct-AWQ",
        "messages": [
            {
                "role": "user",
                "content": "Quel temps fait-il Ã  Paris ?"
            }
        ],
        "tools": tools,
        "tool_choice": "auto",
        "max_tokens": 200
    }
    
    response = requests.post(url, json=payload)
    
    assert response.status_code == 200
    
    data = response.json()
    choice = data["choices"][0]
    
    # VÃ©rifier que le modÃ¨le appelle la fonction
    assert "tool_calls" in choice.get("message", {}), "No tool calls in response"
    
    tool_call = choice["message"]["tool_calls"][0]
    assert tool_call["function"]["name"] == "get_weather"
    
    args = json.loads(tool_call["function"]["arguments"])
    assert "city" in args
    assert "paris" in args["city"].lower()
    
    print("âœ… Test Tool Calling RÃ‰USSI")
    print(f"Fonction appelÃ©e: {tool_call['function']['name']}")
    print(f"Arguments: {args}")
    
    return data

if __name__ == "__main__":
    test_tool_calling_weather()
```

**CritÃ¨res de SuccÃ¨s** :
- âœ… DÃ©tection correcte du besoin de fonction
- âœ… Nom de fonction correct
- âœ… Arguments corrects et formatÃ©s JSON
- âœ… Pas d'hallucinations

---

### Test 7 : Contexte Long (>64k tokens) ðŸ”„ Ã€ FAIRE

**Objectif** : VÃ©rifier support contexte jusqu'Ã  131k tokens

**Script Python** :
```python
# myia_vllm/tests/test_medium_long_context.py

import requests

def test_long_context_64k():
    """Test avec contexte de ~64k tokens"""
    
    url = "http://localhost:5002/v1/chat/completions"
    
    # GÃ©nÃ©rer un long contexte (approximatif : 1 mot = 1.3 tokens)
    # Pour 64k tokens, environ 50k mots
    long_text = "Lorem ipsum dolor sit amet. " * 2000  # ~14k tokens
    
    payload = {
        "model": "Qwen2.5-32B-Instruct-AWQ",
        "messages": [
            {
                "role": "system",
                "content": f"Voici un long document:\n\n{long_text}"
            },
            {
                "role": "user",
                "content": "RÃ©sume ce document en une phrase."
            }
        ],
        "max_tokens": 100
    }
    
    response = requests.post(url, json=payload)
    
    assert response.status_code == 200
    
    data = response.json()
    
    # VÃ©rifier usage tokens
    usage = data.get("usage", {})
    print(f"Tokens prompt: {usage.get('prompt_tokens', 'N/A')}")
    print(f"Tokens completion: {usage.get('completion_tokens', 'N/A')}")
    print(f"Total tokens: {usage.get('total_tokens', 'N/A')}")
    
    answer = data["choices"][0]["message"]["content"]
    assert len(answer) > 0
    
    print("âœ… Test Contexte Long RÃ‰USSI")
    print(f"RÃ©sumÃ©: {answer}")
    
    return data

if __name__ == "__main__":
    test_long_context_64k()
```

**CritÃ¨res de SuccÃ¨s** :
- âœ… Pas d'erreur "context too long"
- âœ… RÃ©ponse cohÃ©rente avec contexte
- âœ… Temps de rÃ©ponse acceptable (<30s)

---

### Test 8 : Streaming Responses ðŸ”„ Ã€ FAIRE

**Objectif** : Tester streaming pour UX temps rÃ©el

**Script Python** :
```python
# myia_vllm/tests/test_medium_streaming.py

import requests

def test_streaming_response():
    """Test streaming de la rÃ©ponse"""
    
    url = "http://localhost:5002/v1/chat/completions"
    
    payload = {
        "model": "Qwen2.5-32B-Instruct-AWQ",
        "messages": [
            {"role": "user", "content": "Raconte-moi une courte histoire."}
        ],
        "max_tokens": 200,
        "stream": True  # Activer streaming
    }
    
    response = requests.post(url, json=payload, stream=True)
    
    assert response.status_code == 200
    
    chunks = []
    for line in response.iter_lines():
        if line:
            line = line.decode('utf-8')
            if line.startswith('data: '):
                data_str = line[6:]  # Remove 'data: ' prefix
                if data_str != '[DONE]':
                    import json
                    chunk = json.loads(data_str)
                    chunks.append(chunk)
                    
                    # Afficher chunk
                    if 'choices' in chunk and len(chunk['choices']) > 0:
                        delta = chunk['choices'][0].get('delta', {})
                        if 'content' in delta:
                            print(delta['content'], end='', flush=True)
    
    print("\nâœ… Test Streaming RÃ‰USSI")
    print(f"Nombre de chunks reÃ§us: {len(chunks)}")
    
    return chunks

if __name__ == "__main__":
    test_streaming_response()
```

**CritÃ¨res de SuccÃ¨s** :
- âœ… RÃ©ception progressive des tokens
- âœ… Pas de latence anormale entre chunks
- âœ… Message complet cohÃ©rent

---

## âš¡ TESTS DE PERFORMANCE

### Test 9 : Latence TTFT (Time To First Token) ðŸ”„ Ã€ FAIRE

**Objectif** : Mesurer temps avant premier token

**Base de rÃ©fÃ©rence** : [`benchmarks/benchmark_latency.py`](../../../benchmarks/benchmark_latency.py)

**Script Python** :
```python
# myia_vllm/tests/test_medium_latency.py

import requests
import time

def test_ttft_latency():
    """Mesurer Time To First Token"""
    
    url = "http://localhost:5002/v1/chat/completions"
    
    payload = {
        "model": "Qwen2.5-32B-Instruct-AWQ",
        "messages": [
            {"role": "user", "content": "Dis bonjour"}
        ],
        "max_tokens": 1,
        "stream": True
    }
    
    start = time.time()
    response = requests.post(url, json=payload, stream=True)
    
    ttft = None
    for line in response.iter_lines():
        if line and ttft is None:
            ttft = time.time() - start
            break
    
    print(f"âœ… TTFT: {ttft:.3f}s")
    
    # Objectifs :
    # - Excellent : < 0.5s
    # - Bon : 0.5-1.0s
    # - Acceptable : 1.0-2.0s
    # - ProblÃ©matique : > 2.0s
    
    if ttft < 0.5:
        print("  ðŸ† Excellent")
    elif ttft < 1.0:
        print("  âœ… Bon")
    elif ttft < 2.0:
        print("  âš ï¸  Acceptable")
    else:
        print("  âŒ ProblÃ©matique")
    
    return ttft

if __name__ == "__main__":
    test_ttft_latency()
```

**CritÃ¨res de SuccÃ¨s** :
- âœ… TTFT < 1.0s (optimal)
- âœ… CohÃ©rent entre runs

---

### Test 10 : Throughput (tokens/sec) ðŸ”„ Ã€ FAIRE

**Objectif** : Mesurer vitesse de gÃ©nÃ©ration

**Script Python** :
```python
# myia_vllm/tests/test_medium_throughput.py

import requests
import time

def test_throughput():
    """Mesurer throughput en tokens/sec"""
    
    url = "http://localhost:5002/v1/chat/completions"
    
    payload = {
        "model": "Qwen2.5-32B-Instruct-AWQ",
        "messages": [
            {"role": "user", "content": "Ã‰cris un paragraphe sur l'IA"}
        ],
        "max_tokens": 500
    }
    
    start = time.time()
    response = requests.post(url, json=payload)
    duration = time.time() - start
    
    data = response.json()
    usage = data.get("usage", {})
    completion_tokens = usage.get("completion_tokens", 0)
    
    throughput = completion_tokens / duration if duration > 0 else 0
    
    print(f"âœ… Throughput: {throughput:.1f} tokens/sec")
    print(f"   Tokens gÃ©nÃ©rÃ©s: {completion_tokens}")
    print(f"   DurÃ©e: {duration:.2f}s")
    
    # Objectifs pour 2x RTX 4090 :
    # - Excellent : > 100 tokens/sec
    # - Bon : 50-100 tokens/sec
    # - Acceptable : 30-50 tokens/sec
    
    if throughput > 100:
        print("  ðŸ† Excellent")
    elif throughput > 50:
        print("  âœ… Bon")
    elif throughput > 30:
        print("  âš ï¸  Acceptable")
    else:
        print("  âŒ Sous-optimal")
    
    return throughput

if __name__ == "__main__":
    test_throughput()
```

**CritÃ¨res de SuccÃ¨s** :
- âœ… Throughput > 50 tokens/sec
- âœ… Utilisation GPU optimale

---

### Test 11 : Tests de Charge (Concurrence) ðŸ”„ Ã€ FAIRE

**Objectif** : Tester comportement sous charge

**Base de rÃ©fÃ©rence** : [`qwen3_benchmark/`](../../../qwen3_benchmark/)

**Script Python** :
```python
# myia_vllm/tests/test_medium_load.py

import requests
import concurrent.futures
import time

def single_request(request_id):
    """Effectuer une requÃªte simple"""
    url = "http://localhost:5002/v1/chat/completions"
    
    payload = {
        "model": "Qwen2.5-32B-Instruct-AWQ",
        "messages": [
            {"role": "user", "content": f"Dis bonjour #{request_id}"}
        ],
        "max_tokens": 50
    }
    
    start = time.time()
    response = requests.post(url, json=payload)
    duration = time.time() - start
    
    return {
        "id": request_id,
        "status": response.status_code,
        "duration": duration
    }

def test_concurrent_load(num_requests=10, max_workers=5):
    """Test avec requÃªtes concurrentes"""
    
    print(f"Lancement de {num_requests} requÃªtes avec {max_workers} workers...")
    
    start = time.time()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(single_request, i) for i in range(num_requests)]
        results = [f.result() for f in concurrent.futures.as_completed(futures)]
    
    total_duration = time.time() - start
    
    # Statistiques
    successes = sum(1 for r in results if r["status"] == 200)
    avg_latency = sum(r["duration"] for r in results) / len(results)
    throughput = num_requests / total_duration
    
    print(f"\nâœ… Test de Charge ComplÃ©tÃ©")
    print(f"   RequÃªtes rÃ©ussies: {successes}/{num_requests}")
    print(f"   Latence moyenne: {avg_latency:.2f}s")
    print(f"   Throughput global: {throughput:.2f} req/sec")
    print(f"   DurÃ©e totale: {total_duration:.2f}s")
    
    return results

if __name__ == "__main__":
    test_concurrent_load(num_requests=20, max_workers=10)
```

**CritÃ¨res de SuccÃ¨s** :
- âœ… 100% de succÃ¨s sous charge modÃ©rÃ©e
- âœ… Latence < 2x latence baseline
- âœ… Pas de crashes ou timeouts

---

### Test 12 : StabilitÃ© Longue DurÃ©e ðŸ”„ Ã€ FAIRE

**Objectif** : VÃ©rifier pas de memory leaks sur 1h+

**Script Python** :
```python
# myia_vllm/tests/test_medium_stability.py

import requests
import time
import psutil
import docker

def test_long_running_stability(duration_minutes=60, interval_seconds=60):
    """Test stabilitÃ© sur longue durÃ©e"""
    
    client = docker.from_env()
    container = client.containers.get("myia-vllm-medium-qwen3")
    
    url = "http://localhost:5002/v1/chat/completions"
    
    start_time = time.time()
    end_time = start_time + (duration_minutes * 60)
    
    iteration = 0
    results = []
    
    print(f"Test de stabilitÃ© sur {duration_minutes} minutes...")
    
    while time.time() < end_time:
        iteration += 1
        
        # RequÃªte test
        try:
            payload = {
                "model": "Qwen2.5-32B-Instruct-AWQ",
                "messages": [
                    {"role": "user", "content": f"Test iteration {iteration}"}
                ],
                "max_tokens": 50
            }
            
            req_start = time.time()
            response = requests.post(url, json=payload, timeout=30)
            latency = time.time() - req_start
            
            # Stats conteneur
            stats = container.stats(stream=False)
            memory_mb = stats['memory_stats']['usage'] / (1024 * 1024)
            
            result = {
                "iteration": iteration,
                "timestamp": time.time(),
                "status": response.status_code,
                "latency": latency,
                "memory_mb": memory_mb
            }
            results.append(result)
            
            print(f"[{iteration}] Status: {response.status_code}, "
                  f"Latency: {latency:.2f}s, Memory: {memory_mb:.0f}MB")
            
        except Exception as e:
            print(f"âŒ Erreur iteration {iteration}: {e}")
            results.append({
                "iteration": iteration,
                "error": str(e)
            })
        
        time.sleep(interval_seconds)
    
    # Analyse rÃ©sultats
    successes = sum(1 for r in results if r.get("status") == 200)
    avg_latency = sum(r["latency"] for r in results if "latency" in r) / len([r for r in results if "latency" in r])
    
    print(f"\nâœ… Test StabilitÃ© ComplÃ©tÃ©")
    print(f"   ItÃ©rations: {iteration}")
    print(f"   SuccÃ¨s: {successes}/{iteration}")
    print(f"   Latence moyenne: {avg_latency:.2f}s")
    
    return results

if __name__ == "__main__":
    # Test court pour validation
    test_long_running_stability(duration_minutes=5, interval_seconds=30)
```

**CritÃ¨res de SuccÃ¨s** :
- âœ… Aucun crash pendant durÃ©e test
- âœ… MÃ©moire stable (pas de leaks)
- âœ… Latence stable dans le temps

---

## ðŸ›¡ï¸ TESTS DE ROBUSTESSE

### Test 13 : Gestion Erreurs API ðŸ”„ Ã€ FAIRE

**Script Python** :
```python
# myia_vllm/tests/test_medium_error_handling.py

import requests

def test_error_invalid_model():
    """Test avec modÃ¨le invalide"""
    url = "http://localhost:5002/v1/chat/completions"
    
    payload = {
        "model": "invalid-model-name",
        "messages": [{"role": "user", "content": "test"}]
    }
    
    response = requests.post(url, json=payload)
    assert response.status_code == 400 or response.status_code == 404
    
    print("âœ… Erreur modÃ¨le invalide gÃ©rÃ©e correctement")

def test_error_empty_messages():
    """Test avec messages vides"""
    url = "http://localhost:5002/v1/chat/completions"
    
    payload = {
        "model": "Qwen2.5-32B-Instruct-AWQ",
        "messages": []
    }
    
    response = requests.post(url, json=payload)
    assert response.status_code == 400
    
    print("âœ… Messages vides rejetÃ©s correctement")

def test_error_excessive_tokens():
    """Test avec max_tokens excessif"""
    url = "http://localhost:5002/v1/chat/completions"
    
    payload = {
        "model": "Qwen2.5-32B-Instruct-AWQ",
        "messages": [{"role": "user", "content": "test"}],
        "max_tokens": 200000  # Au-delÃ  du max
    }
    
    response = requests.post(url, json=payload)
    # Devrait soit rejeter, soit clamper Ã  max_model_len
    
    print(f"âœ… max_tokens excessif gÃ©rÃ© (status: {response.status_code})")

if __name__ == "__main__":
    test_error_invalid_model()
    test_error_empty_messages()
    test_error_excessive_tokens()
```

---

## ðŸ“Š RÃ‰SUMÃ‰ ET PRIORITÃ‰S

### Tests par PrioritÃ©

**CRITIQUE (Ã€ faire en premier)** :
1. âœ… Health Check - **FAIT**
2. âœ… Liste ModÃ¨les - **FAIT**
3. ðŸ”„ Chat Completion Basique
4. ðŸ”„ Text Completion Basique

**HAUTE (Avant production)** :
5. ðŸ”„ Raisonnement Multi-Step
6. ðŸ”„ Tool Calling
7. ðŸ”„ Contexte Long
8. ðŸ”„ Streaming

**MOYENNE (Optimisation)** :
9. ðŸ”„ Latence TTFT
10. ðŸ”„ Throughput
11. ðŸ”„ Tests de Charge

**BASSE (Validation robustesse)** :
12. ðŸ”„ StabilitÃ© Longue DurÃ©e
13. ðŸ”„ Gestion Erreurs
14. ðŸ”„ Limites Context
15. ðŸ”„ RÃ©sistance Crashes

### Ordre d'ExÃ©cution RecommandÃ©

```bash
# 1. Tests basiques (5 min)
python myia_vllm/tests/test_medium_chat_basic.py
python myia_vllm/tests/test_medium_completion_basic.py

# 2. Tests fonctionnels (15 min)
python myia_vllm/tests/test_medium_reasoning.py
python myia_vllm/tests/test_medium_tool_calling.py
python myia_vllm/tests/test_medium_streaming.py

# 3. Tests performance (20 min)
python myia_vllm/tests/test_medium_latency.py
python myia_vllm/tests/test_medium_throughput.py
python myia_vllm/tests/test_medium_load.py

# 4. Tests robustesse (variable)
python myia_vllm/tests/test_medium_error_handling.py
python myia_vllm/tests/test_medium_long_context.py
python myia_vllm/tests/test_medium_stability.py  # 5-60 min
```

---

## ðŸ“ STRUCTURE FICHIERS TESTS

```
myia_vllm/tests/
â”œâ”€â”€ test_medium_chat_basic.py          # Test 3
â”œâ”€â”€ test_medium_completion_basic.py     # Test 4
â”œâ”€â”€ test_medium_reasoning.py            # Test 5
â”œâ”€â”€ test_medium_tool_calling.py         # Test 6
â”œâ”€â”€ test_medium_long_context.py         # Test 7
â”œâ”€â”€ test_medium_streaming.py            # Test 8
â”œâ”€â”€ test_medium_latency.py              # Test 9
â”œâ”€â”€ test_medium_throughput.py           # Test 10
â”œâ”€â”€ test_medium_load.py                 # Test 11
â”œâ”€â”€ test_medium_stability.py            # Test 12
â””â”€â”€ test_medium_error_handling.py       # Test 13
```

---

## ðŸŽ¯ MÃ‰TRIQUES CIBLES

### Performance (2x RTX 4090, Qwen3-32B-AWQ)

| MÃ©trique | Cible Minimale | Cible Optimale | Notes |
|----------|----------------|----------------|-------|
| TTFT | < 1.0s | < 0.5s | Prompt court |
| Throughput | > 50 tok/s | > 100 tok/s | GÃ©nÃ©ration |
| Latence P50 | < 2.0s | < 1.0s | Prompts variÃ©s |
| Latence P99 | < 5.0s | < 3.0s | Worst case |
| Concurrent Requests | 10 | 20+ | Sans dÃ©gradation |

### Robustesse

| CritÃ¨re | Cible |
|---------|-------|
| Uptime sur 1h | 100% |
| Taux erreurs | < 0.1% |
| Memory leaks | Aucun |
| Crash recovery | < 30s |

---

## ðŸ“ NOTES IMPORTANTES

### Contexte du Document

Ce plan de tests a Ã©tÃ© crÃ©Ã© durant la **PHASE 9** de la MISSION 9 (RedÃ©ploiement Service Medium - MÃ©thodologie SDDD).

**Raison de non-exÃ©cution** : Selon les instructions SDDD, la phase 9 consiste Ã  **documenter** les tests sans les exÃ©cuter. L'exÃ©cution sera faite ultÃ©rieurement par l'Ã©quipe ou via une sous-tÃ¢che dÃ©diÃ©e.

### PrÃ©requis Avant ExÃ©cution

1. âœ… Service medium dÃ©ployÃ© et HEALTHY
2. âœ… Port 5002 accessible
3. âœ… Python 3.8+ avec `requests` installÃ©
4. âœ… Docker accessible pour stats conteneur
5. ðŸ”„ CrÃ©er les fichiers de tests listÃ©s ci-dessus

### DÃ©pendances Python

```bash
pip install requests pytest docker psutil
```

### Variables d'Environnement

```bash
# Optionnel pour tests
export VLLM_API_BASE=http://localhost:5002
export VLLM_MODEL=Qwen2.5-32B-Instruct-AWQ
```

---

## ðŸ”— RÃ‰FÃ‰RENCES

- DÃ©ploiement : [`DEPLOYMENT_MEDIUM_20251016.md`](./DEPLOYMENT_MEDIUM_20251016.md)
- Configuration : [`MEDIUM_SERVICE.md`](./MEDIUM_SERVICE.md)
- ParamÃ¨tres : [`MEDIUM_SERVICE_PARAMETERS.md`](../docker/MEDIUM_SERVICE_PARAMETERS.md)
- Tests existants : [`tests/scripts/tests/`](../../tests/scripts/tests/)
- Benchmarks : [`qwen3_benchmark/`](../../../qwen3_benchmark/)

---

**Version** : 1.0  
**Status** : âš ï¸ Documentation uniquement - Tests non exÃ©cutÃ©s  
**DerniÃ¨re mise Ã  jour** : 2025-10-16  
**Auteur** : Agent Code (Mode SDDD - Mission 9)