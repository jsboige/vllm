# Docker Compose profile for Qwen3-Coder-Next (80B MoE)
# Migration from Qwen3-32B-AWQ for agentic use with Claude Code
#
# Hardware: 3x RTX 4090 (72GB VRAM total)
# Model: W4A16 4-bit quantization (~46GB) from HuggingFace
# Quant format: compressed-tensors (auto-detected by vLLM)
# Source: https://huggingface.co/cyankiwi/Qwen3-Coder-Next-AWQ-4bit
# Calibration: nvidia/Nemotron-SWE-v1 (Software Engineering dataset - optimal for code)
#
# DEPLOYMENT NOTES:
#   - Uses Pipeline Parallelism (PP=3) not Tensor Parallelism (TP=3)
#   - TP=3 fails: model intermediate_size (8192) not divisible by 3
#   - TP=2 fails: 48GB insufficient for ~46GB model + KV cache
#   - PP=3 distributes layers across GPUs (~22GB each), leaves ~2GB for KV
#
# Context: 65K tokens with FP8 KV cache
# Performance: ~5-6 tok/s single user, ~21 tok/s aggregate (5 concurrent)

services:
  vllm-medium-coder:
    image: vllm/vllm-openai:latest  # Requires >= v0.15.0 for Qwen3-Coder-Next support
    container_name: myia_vllm-medium-coder
    command: >
      --host 0.0.0.0
      --port ${VLLM_PORT_CODER:-5002}
      --model ${VLLM_MODEL_CODER:-cyankiwi/Qwen3-Coder-Next-AWQ-4bit}
      --api-key ${VLLM_API_KEY_MEDIUM}

      --tensor-parallel-size 1
      --pipeline-parallel-size 3
      --gpu-memory-utilization 0.88
      --max-model-len 65536
      --kv-cache-dtype fp8
      --enable-chunked-prefill
      --dtype half
      --enable-auto-tool-choice
      --tool-call-parser qwen3_coder
      --swap-space 16
      --enforce-eager
      --distributed-executor-backend mp
      --disable-custom-all-reduce
    ipc: host
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
    volumes:
      - ${VLLM_MODELS_PATH:-./models}:/models:ro
      - ${HF_CACHE_PATH:-~/.cache/huggingface}:/root/.cache/huggingface
    ports:
      - "${VLLM_PORT_CODER:-5002}:${VLLM_PORT_CODER:-5002}"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${VLLM_PORT_CODER:-5002}/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 600s  # 10 minutes for 80B MoE model loading
    restart: unless-stopped

# Alternative configurations (commented):
#
# === WITHOUT FP8 KV CACHE (if bug #26646 not fixed) ===
# Replace --kv-cache-dtype fp8 with:
#   --max-model-len 98304
# And remove the --kv-cache-dtype line
#
# === MINIMUM CONTEXT (64K) ===
# If OOM persists:
#   --max-model-len 65536
#   --gpu-memory-utilization 0.85
#
# === EXPERT OFFLOADING (experimental) ===
# For CPU offloading of some experts:
#   --cpu-offload-gb 8
