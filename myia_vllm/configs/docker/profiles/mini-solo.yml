# Docker Compose profile for Qwen3-VL-8B-Thinking (Solo Mode - Maximum Performance)
# Deployed ALONE on GPU 2 with full optimization
#
# Hardware: 1x RTX 4090 (24GB VRAM, dedicated)
# Model: AWQ 4-bit quantization (~5.5GB) from HuggingFace
# Quant format: compressed-tensors (W4A16, auto-detected by vLLM)
# Source: https://huggingface.co/cyankiwi/Qwen3-VL-8B-Thinking-AWQ-4bit
#
# SOLO MODE BENEFITS (vs shared GPU):
#   - CUDA graphs ENABLED (+40-70% throughput vs enforce-eager)
#   - gpu-memory-utilization 0.88 (vs 0.45 shared)
#   - max-model-len 131072 (vs 4096 shared) - native 256K, 128K fits in 24GB
#   - max-num-batched-tokens 32768 (vs 4096 shared)
#   - Expected: 80-120 tok/s single, 200-350 tok/s concurrent
#
# NOTE: This is the dedicated GPU 2 service (micro is deprecated).
#       GPU 2 is now exclusively allocated to Qwen3-VL-8B-Thinking.
#
# Features: Vision (images), Thinking/Reasoning (chain-of-thought)
# Context: 128K tokens with FP8 KV cache (native 256K, limited by VRAM)
#
# Build: not needed (uses standard vllm-openai image)
# Run:   docker compose -f myia_vllm/configs/docker/profiles/mini-solo.yml --env-file myia_vllm/.env up -d

services:
  vllm-mini-solo:
    image: vllm/vllm-openai:latest
    container_name: myia_vllm-mini-solo
    command: >
      --host 0.0.0.0
      --port ${VLLM_PORT_MINI:-5001}
      --model ${VLLM_MODEL_MINI:-cyankiwi/Qwen3-VL-8B-Thinking-AWQ-4bit}
      --served-model-name qwen3-vl-8b-thinking
      --api-key ${VLLM_API_KEY_MINI}

      --gpu-memory-utilization 0.88
      --max-model-len 131072
      --max-num-batched-tokens 32768
      --kv-cache-dtype fp8
      --dtype half
      --max-num-seqs 64

      --enable-auto-tool-choice
      --tool-call-parser hermes
      --reasoning-parser deepseek_r1

      --distributed-executor-backend mp
      --disable-log-requests

      --limit-mm-per-prompt '{"image":4,"video":0}'
      --mm-processor-kwargs '{"max_pixels":774000}'
      --skip-mm-profiling
    ipc: host
    shm_size: '8gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['2']
              capabilities: [gpu]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - CUDA_VISIBLE_DEVICES=2
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      - OMP_NUM_THREADS=8
      - VLLM_MARLIN_USE_ATOMIC_ADD=1
    volumes:
      - ${VLLM_MODELS_PATH:-./models}:/models:ro
      - ${HF_CACHE_PATH:-~/.cache/huggingface}:/root/.cache/huggingface
    ports:
      - "${VLLM_PORT_MINI:-5001}:${VLLM_PORT_MINI:-5001}"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${VLLM_PORT_MINI:-5001}/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 240s  # 4 minutes for 8B VL model + CUDA graph capture
    restart: unless-stopped

# =====================================================================
# SOLO MODE PERFORMANCE EXPECTATIONS
# =====================================================================
#
# Based on internet benchmarks for Qwen3-VL-8B AWQ on RTX 4090:
#
# | Config                | Single User | 5 Concurrent | Source                |
# |-----------------------|-------------|--------------|----------------------|
# | Solo (CUDA graphs)    | 80-120 tok/s | 200-350 tok/s| Expected optimal    |
# | Shared (enforce-eager)| 53-54 tok/s  | 70 tok/s     | Our current results |
#
# Delta: +50-120% single, +185-400% concurrent
#
# === VISION MODEL SPECIFICS ===
# Vision encoder (ViT) adds ~0.6 GB to VRAM usage.
# Image tokens are expensive: ~774 pixels = ~1K tokens per image
# limit-mm-per-prompt: up to 4 images at 774×774 resolution
# Video disabled (video=0) to conserve memory.
#
# === KV CACHE BUDGET ===
# gpu-memory-utilization=0.88 → 21.1 GB available
# Model weights (AWQ 4-bit): ~5.5 GB
# Vision encoder (BF16): ~0.6 GB
# CUDA graphs overhead: ~4-6 GB
# Available for KV cache: ~9-11 GB
# At FP8 (28 KB/token): ~330K tokens theoretical capacity
# Configured at 128K context (enough for Playwright snapshots + vision)
# 128K = 3.5 GB KV, fits comfortably in 9 GB budget
#
# === THINKING MODE ===
# deepseek_r1 parser extracts <think>...</think> automatically.
# Chain-of-thought reasoning for visual analysis.
#
# === IF OOM ON STARTUP ===
# 1. Reduce gpu-memory-utilization to 0.85
# 2. Reduce max-model-len to 8192
# 3. Reduce --limit-mm-per-prompt '{"image":2,"video":0}'
# 4. As last resort: add --enforce-eager (loses CUDA graph benefit)
