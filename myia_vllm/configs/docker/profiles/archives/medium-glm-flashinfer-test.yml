# ARCHIVED: 2026-02-17 - FlashInfer optimization test (ADOPTED)
#
# Docker Compose profile for GLM-4.7-Flash (31B MoE, 3B active)
# TEST VARIANT: FlashInfer backend testing
#
# This was a temporary test configuration to benchmark FlashInfer backend
# against the baseline TRITON_MLA configuration.
#
# ADDED VARIABLES (vs baseline):
#   - VLLM_USE_FLASHINFER_MOE_FP16=1 (recommended by QuantTrio AWQ creator)
#   - VLLM_ATTENTION_BACKEND=FLASHINFER (recommended by NVIDIA forums)
#
# TEST RESULTS (2026-02-17):
#   FlashInfer variant: 44.6 tok/s decode, 183.8 tok/s concurrent (+13% vs baseline)
#   Baseline: 39.5 tok/s decode, 163.3 tok/s concurrent
#   Tool latency: 1858ms (FlashInfer) vs 2550ms (baseline) = -27%
#   30K TTFT cold: 15.2s (FlashInfer) vs 19.1s (baseline) = -20%
#
# OUTCOME: VLLM_USE_FLASHINFER_MOE_FP16=1 adopted in medium-glm.yml
#          VLLM_ATTENTION_BACKEND=FLASHINFER rejected (unknown variable, no effect)
#
# This file has been archived and superseded by medium-glm.yml with FlashInfer enabled.
#
# Hardware: 2x RTX 4090 (48GB VRAM) - frees GPU 2 for other services
# Model: AWQ 4-bit quantization (~19GB) from HuggingFace
# Source: https://huggingface.co/QuantTrio/GLM-4.7-Flash-AWQ
#
# DEPLOYMENT NOTES:
#   - Uses Tensor Parallelism (TP=2) + Expert Parallelism on GPUs 0,1
#   - AWQ 4-bit: ~9.5GB/GPU, leaves headroom for KV cache + CUDA graphs
#   - MLA (Multi-Latent Attention) enables very efficient KV cache (54 KB/token)
#   - Requires vLLM nightly + transformers >= 5.0 (custom Dockerfile)
#   - V1 engine: chunked prefill + CUDA graphs + torch.compile by default
#
# Context: 128K tokens with MLA (very compact KV cache, ~54 KB/token)
# NOTE: FP8 KV cache NOT supported with MLA on Ada Lovelace (RTX 4090)
#   TRITON_MLA is the only working MLA backend on SM89
#
# Build: docker compose -f myia_vllm/configs/docker/profiles/medium-glm-flashinfer-test.yml build
# Run:   docker compose -f myia_vllm/configs/docker/profiles/medium-glm-flashinfer-test.yml --env-file myia_vllm/.env up -d
# Warm:  python myia_vllm/scripts/testing/warmup_glm.py  (after startup)

services:
  vllm-medium-glm:
    build:
      context: ..
      dockerfile: Dockerfile.glm-flash
    container_name: myia_vllm-medium-glm
    command: >
      --host 0.0.0.0
      --port ${VLLM_PORT_GLM:-5002}
      --model ${VLLM_MODEL_GLM:-QuantTrio/GLM-4.7-Flash-AWQ}
      --served-model-name glm-4.7-flash
      --api-key ${VLLM_API_KEY_MEDIUM}

      --tensor-parallel-size 2
      --enable-expert-parallel
      --gpu-memory-utilization 0.92
      --max-model-len 131072
      --kv-cache-dtype auto
      --dtype half
      --max-num-batched-tokens 32768
      --max-num-seqs 64
      --enable-prefix-caching
      --enable-auto-tool-choice
      --tool-call-parser glm47
      --reasoning-parser glm45
      --distributed-executor-backend mp
      --disable-log-requests
      --middleware logging_middleware.RequestResponseLogger
    ipc: host
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0', '1']
              capabilities: [gpu]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      - VLLM_MARLIN_USE_ATOMIC_ADD=1
      - VLLM_USE_DEEP_GEMM=0
      - OMP_NUM_THREADS=4
      - VLLM_ENABLE_INDUCTOR_MAX_AUTOTUNE=1
      - VLLM_ENABLE_INDUCTOR_COORDINATE_DESCENT_TUNING=1
      - VLLM_FLOAT32_MATMUL_PRECISION=medium
      - PYTHONPATH=/middleware
      - VLLM_LOG_DIR=/logs
      # === FLASHINFER TEST VARIABLES ===
      - VLLM_USE_FLASHINFER_MOE_FP16=1
      - VLLM_ATTENTION_BACKEND=FLASHINFER
    volumes:
      - ${VLLM_MODELS_PATH:-./models}:/models:ro
      - ${HF_CACHE_PATH:-~/.cache/huggingface}:/root/.cache/huggingface
      - vllm-compile-cache:/root/.cache/vllm
      - ./../../../middleware:/middleware:ro
      - ${VLLM_LOG_PATH:-./../../../logs}:/logs
    ports:
      - "${VLLM_PORT_GLM:-5002}:${VLLM_PORT_GLM:-5002}"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${VLLM_PORT_GLM:-5002}/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s  # 5 minutes for model loading + CUDA graph capture
    restart: unless-stopped

volumes:
  vllm-compile-cache:  # Persists torch.compile cache between container restarts
