services:
  vllm-medium-qwen3:
    image: vllm/vllm-openai:latest
    container_name: myia_vllm-medium-qwen3
    command: >
      --host 0.0.0.0
      --port ${VLLM_PORT_MEDIUM:-5002}
      --model Qwen/Qwen3-32B-AWQ
      --api-key ${VLLM_API_KEY_MEDIUM}
      --tensor-parallel-size 2
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION_MEDIUM:-0.95}
      --max-model-len 131072
      --quantization awq_marlin
      --kv-cache-dtype fp8
      --enable-prefix-caching
      --enable-chunked-prefill
      --dtype ${DTYPE_MEDIUM:-half}
      --enable-auto-tool-choice
      --tool-call-parser qwen3_xml
      --reasoning-parser qwen3
      --distributed-executor-backend=mp
      --rope_scaling '{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}'
      --swap-space 16
    runtime: nvidia
    ipc: host
    shm_size: '16gb'
    environment:
      - NVIDIA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES_MEDIUM:-0,1}
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
    ports:
      - "${VLLM_PORT_MEDIUM:-5002}:${VLLM_PORT_MEDIUM:-5002}"
    healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:${VLLM_PORT_MEDIUM:-5002}/health"]
        interval: 30s
        timeout: 10s
        retries: 5
        start_period: 300s