services:
  vllm-medium-qwen3:
    image: vllm/vllm-openai:latest
    container_name: myia-vllm-medium-qwen3
    command: >
      --host 0.0.0.0
      --port ${VLLM_PORT_MEDIUM:-5002}
      --model Qwen/Qwen3-32B-AWQ
      --api-key ${VLLM_API_KEY_MEDIUM}
      --tensor-parallel-size 2
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION_MEDIUM:-0.95}
      --max-model-len 131072
      --quantization awq_marlin
      --kv_cache_dtype fp8
      --dtype ${DTYPE_MEDIUM:-half}
      --enable-auto-tool-choice
      --tool-call-parser hermes
      --reasoning-parser qwen3
      --distributed-executor-backend=mp
      --rope_scaling '{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}'
      --swap-space 16
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ['${CUDA_VISIBLE_DEVICES_MEDIUM}']
    healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:${VLLM_PORT_MEDIUM:-5002}/health"]
        interval: 30s
        timeout: 10s
        retries: 5
        start_period: 300s