# Docker Compose profile for ZwZ-8B (DEFAULT vision model on GPU 2)
# Qwen3-VL-8B finetune specialized for fine-grained visual perception
# Deployed ALONE on GPU 2 with full optimization
#
# Hardware: 1x RTX 4090 (24GB VRAM, dedicated)
# Model: AWQ 4-bit quantization (~5.5GB) - CREATED VIA quantize_zwz_8b.py
# Quant format: compressed-tensors (W4A16, auto-detected by vLLM)
# Source: https://huggingface.co/inclusionAI/ZwZ-8B
#
# STATUS: DEFAULT vision model since 2026-02-16
#   - Replaces Qwen3-VL-8B-Thinking (mini-solo.yml, still available as fallback)
#   - GLM-4.6V-Flash tested and rejected (vLLM incompatible, see archives/)
#
# PERFORMANCE OPTIMIZATIONS:
#   1. CUDA graphs + torch.compile enabled (+40-70% throughput vs enforce-eager)
#   2. Prefix caching (10x TTFT reduction for repeated prompts)
#   3. Persistent compile cache (vllm-compile-cache volume)
#   4. FP8 KV cache (28 KB/token)
#   5. Marlin AWQ optimizations (VLLM_MARLIN_USE_ATOMIC_ADD=1)
#   6. DISABLED: Inductor autotune (regresses on vLLM dev216)
#   7. DISABLED: Logging middleware (causes throughput loss in batching)
#   8. gpu-memory-utilization 0.88
#   9. max-model-len 131072 (native 256K, 128K fits in 24GB)
#  10. max-num-batched-tokens 32768
#
# Features: Vision (images), Fine-grained visual perception, Tool calling (hermes)
# Context: 128K tokens with FP8 KV cache (native 256K, limited by VRAM)
#
# Build: not needed (uses standard vllm-openai image)
# Run:   docker compose -f myia_vllm/configs/docker/profiles/mini-zwz.yml --env-file myia_vllm/.env up -d

services:
  vllm-mini-zwz:
    image: vllm/vllm-openai:nightly
    container_name: myia_vllm-mini-zwz
    command: >
      --host 0.0.0.0
      --port ${VLLM_PORT_MINI:-5001}
      --model /models/ZwZ-8B-AWQ-4bit
      --served-model-name zwz-8b
      --api-key ${VLLM_API_KEY_MINI}

      --gpu-memory-utilization 0.88
      --max-model-len 131072
      --max-num-batched-tokens 32768
      --kv-cache-dtype fp8
      --dtype half
      --max-num-seqs 64

      --enable-auto-tool-choice
      --tool-call-parser hermes
      --enable-prefix-caching

      --distributed-executor-backend mp
      --disable-log-requests

      --limit-mm-per-prompt '{"image":4,"video":0}'
      --mm-processor-kwargs '{"max_pixels":774000}'
      --skip-mm-profiling
    ipc: host
    shm_size: '8gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['2']
              capabilities: [gpu]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - CUDA_VISIBLE_DEVICES=2
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      - OMP_NUM_THREADS=8
      - VLLM_MARLIN_USE_ATOMIC_ADD=1
      # INDUCTOR AUTOTUNE DISABLED - causes regression on vLLM dev216 (tested on GLM, -18% decode)
      # - VLLM_ENABLE_INDUCTOR_MAX_AUTOTUNE=1
      # - VLLM_ENABLE_INDUCTOR_COORDINATE_DESCENT_TUNING=1
      # - VLLM_FLOAT32_MATMUL_PRECISION=medium
      # MIDDLEWARE DISABLED - causes 40-65% performance loss on GLM batching
      # - PYTHONPATH=/middleware
      # - VLLM_LOG_DIR=/logs
    volumes:
      - d:/vllm/models:/models:ro
      - hf-cache:/root/.cache/huggingface
      - vllm-compile-cache:/root/.cache/vllm
      - ./../../../middleware:/middleware:ro
      - ${VLLM_LOG_PATH:-./../../../logs}:/logs
    working_dir: /app
    ports:
      - "${VLLM_PORT_MINI:-5001}:${VLLM_PORT_MINI:-5001}"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${VLLM_PORT_MINI:-5001}/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 240s  # 4 minutes for 8B VL model + CUDA graph capture
    restart: unless-stopped

# =====================================================================
# ZwZ-8B SPECIFICS
# =====================================================================
#
# ZwZ-8B is a finetune of Qwen3-VL-8B-Instruct specialized for:
#   - Fine-grained visual perception
#   - Single-pass inference (no iterative zooming like "Thinking-with-Images")
#   - Out-of-distribution generalization
#
# Key differences from Qwen3-VL-8B-Thinking:
#   - NO reasoning/thinking mode (no deepseek_r1 parser)
#   - Optimized for detailed visual analysis in one forward pass
#   - Trained with Region-to-Image Distillation from Qwen3-VL-235B and GLM-4.5V
#
# Training data: inclusionAI/ZwZ-RL-VQA (74K VQA samples)
#
# === VISION MODEL SPECIFICS ===
# Vision encoder (ViT) adds ~0.6 GB to VRAM usage.
# Image tokens: ~774 pixels = ~1K tokens per image
# limit-mm-per-prompt: up to 4 images at 774×774 resolution
# Video disabled (video=0) to conserve memory.
#
# === KV CACHE BUDGET ===
# gpu-memory-utilization=0.88 → 21.1 GB available
# Model weights (AWQ 4-bit): ~5.5 GB
# Vision encoder (BF16): ~0.6 GB
# CUDA graphs overhead: ~4-6 GB
# Available for KV cache: ~9-11 GB
# At FP8 (28 KB/token): ~330K tokens theoretical capacity
# Configured at 128K context
#
# === IF OOM ON STARTUP ===
# 1. Reduce gpu-memory-utilization to 0.85
# 2. Reduce max-model-len to 8192
# 3. Reduce --limit-mm-per-prompt '{"image":2,"video":0}'
# 4. As last resort: add --enforce-eager (loses CUDA graph benefit)
#
# === SWITCHING TO mini-solo (Qwen3-VL-Thinking) ===
# ZwZ-8B is the default. To temporarily switch to Qwen3-VL-Thinking:
#   docker compose -f myia_vllm/configs/docker/profiles/mini-zwz.yml down
#   docker compose -f myia_vllm/configs/docker/profiles/mini-solo.yml --env-file myia_vllm/.env up -d
#

volumes:
  hf-cache:
  vllm-compile-cache:  # Persists torch.compile cache between container restarts
