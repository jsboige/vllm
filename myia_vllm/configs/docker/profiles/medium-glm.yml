# Docker Compose profile for GLM-4.7-Flash (31B MoE, 3B active)
# Optimized for maximum performance on 2x RTX 4090
#
# Hardware: 2x RTX 4090 (48GB VRAM) - frees GPU 2 for other services
# Model: AWQ 4-bit quantization (~19GB) from HuggingFace
# Source: https://huggingface.co/QuantTrio/GLM-4.7-Flash-AWQ
#
# DEPLOYMENT NOTES:
#   - Uses Tensor Parallelism (TP=2) + Expert Parallelism on GPUs 0,1
#   - AWQ 4-bit: ~9.5GB/GPU, leaves headroom for KV cache + CUDA graphs
#   - MLA (Multi-Latent Attention) enables very efficient KV cache (54 KB/token)
#   - Requires vLLM nightly + transformers >= 5.0 (custom Dockerfile)
#   - V1 engine: chunked prefill + CUDA graphs + torch.compile by default
#
# Context: 128K tokens with MLA (very compact KV cache, ~54 KB/token)
# NOTE: FP8 KV cache NOT supported with MLA on Ada Lovelace (RTX 4090)
#   TRITON_MLA is the only working MLA backend on SM89
#
# PERFORMANCE OPTIMIZATIONS APPLIED:
#   1. CUDA graphs + torch.compile enabled (3-4x speedup over enforce-eager)
#   2. No --enable-chunked-prefill flag (prevents V0 fallback, V1 does it auto)
#   3. --enable-expert-parallel → reduces cross-GPU MoE communication
#   4. VLLM_MARLIN_USE_ATOMIC_ADD=1 → optimized AWQ kernel accumulation
#   5. 128K context (MLA capacity: ~240K tokens at 0.92 GPU util)
#   6. No --swap-space (V1 uses recompute, not swap)
#   7. --max-num-batched-tokens 32768 for better MoE batching
#   8. Post-startup warmup recommended (see warmup_glm.py)
#   9. INDUCTOR_MAX_AUTOTUNE + COORDINATE_DESCENT → +30% concurrent throughput
#  10. FLOAT32_MATMUL_PRECISION=medium → TF32 for residual FP32 ops
#
# Build: docker compose -f myia_vllm/configs/docker/profiles/medium-glm.yml build
# Run:   docker compose -f myia_vllm/configs/docker/profiles/medium-glm.yml --env-file myia_vllm/.env up -d
# Warm:  python myia_vllm/scripts/testing/warmup_glm.py  (after startup)

services:
  vllm-medium-glm:
    build:
      context: ..
      dockerfile: Dockerfile.glm-flash
    container_name: myia_vllm-medium-glm
    command: >
      --host 0.0.0.0
      --port ${VLLM_PORT_GLM:-5002}
      --model ${VLLM_MODEL_GLM:-QuantTrio/GLM-4.7-Flash-AWQ}
      --served-model-name glm-4.7-flash
      --api-key ${VLLM_API_KEY_MEDIUM}

      --tensor-parallel-size 2
      --enable-expert-parallel
      --gpu-memory-utilization 0.92
      --max-model-len 131072
      --kv-cache-dtype auto
      --dtype half
      --max-num-batched-tokens 32768
      --max-num-seqs 64
      --enable-prefix-caching
      --enable-auto-tool-choice
      --tool-call-parser glm47
      --reasoning-parser glm45
      --distributed-executor-backend mp
      --disable-log-requests
    ipc: host
    shm_size: '16gb'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0', '1']
              capabilities: [gpu]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - CUDA_DEVICE_ORDER=PCI_BUS_ID
      - VLLM_MARLIN_USE_ATOMIC_ADD=1
      - VLLM_USE_DEEP_GEMM=0
      - OMP_NUM_THREADS=4
      - VLLM_ENABLE_INDUCTOR_MAX_AUTOTUNE=1
      - VLLM_ENABLE_INDUCTOR_COORDINATE_DESCENT_TUNING=1
      - VLLM_FLOAT32_MATMUL_PRECISION=medium
    volumes:
      - ${VLLM_MODELS_PATH:-./models}:/models:ro
      - ${HF_CACHE_PATH:-~/.cache/huggingface}:/root/.cache/huggingface
      - vllm-compile-cache:/root/.cache/vllm
    ports:
      - "${VLLM_PORT_GLM:-5002}:${VLLM_PORT_GLM:-5002}"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${VLLM_PORT_GLM:-5002}/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s  # 5 minutes for model loading + CUDA graph capture
    restart: unless-stopped

volumes:
  vllm-compile-cache:  # Persists torch.compile cache between container restarts

# =====================================================================
# OPTIMIZATION NOTES
# =====================================================================
#
# === V1 ENGINE (active by default on vLLM nightly) ===
# V1 engine provides: async scheduling, piecewise CUDA graphs,
# automatic chunked prefill, recompute-based preemption.
# Do NOT add --enable-chunked-prefill or --num-scheduler-steps
# (these force V0 fallback).
#
# === CUDA GRAPHS + TORCH.COMPILE (critical: 3-4x speedup) ===
# Without CUDA graphs (enforce-eager): 12-13 tok/s decode, 2.6K tok/s prefill
# With CUDA graphs + autotune: 55 tok/s decode, 216 tok/s concurrent (5 users)
# CUDA graphs consume ~4-6.5 GiB per GPU on RTX 4090.
# First startup is slower (graph capture + torch.compile warmup).
# First 2-3 requests after startup have elevated TTFT (10-14s) due to
# graph capture for new input shapes. Run warmup_glm.py after startup
# to pre-trigger these captures before real users connect.
# Do NOT use --enforce-eager (tested: 3-4x slower on all metrics).
#
# === EXPERT PARALLELISM ===
# With TP=2: each GPU holds full weights of 32/64 experts.
# Only 4 experts active per token, high locality on each GPU.
# Reduces cross-GPU communication on PCIe (no NVLink).
#
# === CONTEXT WINDOW ===
# MLA KV cache: ~54 KB/token. At 0.92 util with CUDA graphs:
# ~6-7 GiB per GPU for KV = ~120-130K tokens capacity.
# Max viable: 0.92 (0.95 OOM: only 22.26/23.99 GiB free at startup)
# 131K fits for single requests, concurrent requests share the pool.
#
# === MTP SPECULATIVE DECODING (NOT recommended with AWQ) ===
# MTP has 0% acceptance rate at 4-bit precision (tested on GLM-4.6-AWQ).
# Only viable with FP8 or FP16 models. Do NOT add:
#   --speculative-config '{"method":"mtp","num_speculative_tokens":1}'
#
# === INDUCTOR AUTOTUNE (tested: +30% concurrent throughput) ===
# VLLM_ENABLE_INDUCTOR_MAX_AUTOTUNE=1 + COORDINATE_DESCENT_TUNING=1
# torch.compile tries more kernel variants, finding faster paths.
# First compile is slower (minutes), but persisted in vllm-compile-cache.
# VLLM_FLOAT32_MATMUL_PRECISION=medium enables TF32 for residual FP32 ops.
# TESTED AND REJECTED:
#   --block-size 32: -7% concurrent (worse memory locality for MoE)
#   VLLM_DISABLE_SHARED_EXPERTS_STREAM=0: -8% concurrent, +90% tool latency
#
# === FP8 MODEL ALTERNATIVE (higher quality, less context) ===
# Replace model with: marksverdhei/GLM-4.7-Flash-FP8
# Model is ~30GB (~15GB/GPU), less KV headroom
# FP8 enables MTP speculative decoding (worth testing)
