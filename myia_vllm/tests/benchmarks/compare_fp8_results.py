#!/usr/bin/env python3
"""
Script de comparaison des r√©sultats de benchmarks FP8
Objectif: Analyser l'impact de --calculate-kv-scales sur performance et qualit√©
"""

import json
import sys
import argparse
from pathlib import Path
from typing import Dict, List, Optional

def load_benchmark_results(file_path: str) -> Optional[Dict]:
    """Charger un fichier de r√©sultats de benchmark"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"‚ùå Fichier non trouv√©: {file_path}")
        return None
    except json.JSONDecodeError as e:
        print(f"‚ùå Erreur JSON dans {file_path}: {e}")
        return None

def calculate_metrics_difference(baseline: Dict, calibrated: Dict) -> Dict:
    """Calculer les diff√©rences de m√©triques entre baseline et calibr√©"""
    diff = {}
    
    # M√©triques TTFT
    if 'ttft' in baseline and 'ttft' in calibrated:
        baseline_ttft = baseline['ttft']['duration_ms']
        calibrated_ttft = calibrated['ttft']['duration_ms']
        diff['ttft'] = {
            'baseline_ms': baseline_ttft,
            'calibrated_ms': calibrated_ttft,
            'difference_ms': calibrated_ttft - baseline_ttft,
            'difference_percent': ((calibrated_ttft - baseline_ttft) / baseline_ttft) * 100 if baseline_ttft > 0 else 0
        }
    
    # M√©triques Throughput
    if 'throughput' in baseline and 'throughput' in calibrated:
        baseline_tps = baseline['throughput']['tokens_per_second']
        calibrated_tps = calibrated['throughput']['tokens_per_second']
        diff['throughput'] = {
            'baseline_tps': baseline_tps,
            'calibrated_tps': calibrated_tps,
            'difference_tps': calibrated_tps - baseline_tps,
            'difference_percent': ((calibrated_tps - baseline_tps) / baseline_tps) * 100 if baseline_tps > 0 else 0
        }
    
    return diff

def analyze_warnings(baseline: Dict, calibrated: Dict) -> Dict:
    """Analyser les warnings entre baseline et calibr√©"""
    analysis = {
        'baseline_warnings': baseline.get('warnings_observed', []),
        'calibrated_warnings': calibrated.get('warnings_observed', []),
        'warnings_resolved': [],
        'warnings_remaining': []
    }
    
    baseline_warnings = set(analysis['baseline_warnings'])
    calibrated_warnings = set(analysis['calibrated_warnings'])
    
    # Warnings r√©solus par calibration
    analysis['warnings_resolved'] = list(baseline_warnings - calibrated_warnings)
    
    # Warnings restants apr√®s calibration
    analysis['warnings_remaining'] = list(calibrated_warnings)
    
    return analysis

def generate_comparison_report(baseline_file: str, calibrated_file: str, output_file: str):
    """G√©n√©rer un rapport de comparaison complet"""
    
    print(f"üîç Analyse comparaison FP8")
    print(f"   Baseline: {baseline_file}")
    print(f"   Calibr√©: {calibrated_file}")
    print(f"   Sortie: {output_file}")
    
    # Charger les r√©sultats
    baseline_results = load_benchmark_results(baseline_file)
    calibrated_results = load_benchmark_results(calibrated_file)
    
    if not baseline_results or not calibrated_results:
        print("‚ùå Impossible de charger les fichiers de r√©sultats")
        return False
    
    # Calculer les diff√©rences
    metrics_diff = calculate_metrics_difference(baseline_results, calibrated_results)
    warnings_analysis = analyze_warnings(baseline_results, calibrated_results)
    
    # G√©n√©rer le rapport
    report = {
        "comparison_metadata": {
            "timestamp": "2025-10-30T12:48:00Z",
            "baseline_file": baseline_file,
            "calibrated_file": calibrated_file,
            "model": baseline_results.get('model', 'Unknown'),
            "objective": "Analyser impact de --calculate-kv-scales sur FP8 KV cache"
        },
        "performance_impact": metrics_diff,
        "warnings_analysis": warnings_analysis,
        "recommendations": []
    }
    
    # G√©n√©rer recommandations bas√©es sur les r√©sultats
    recommendations = []
    
    # Analyse TTFT
    if 'ttft' in metrics_diff:
        ttft_diff = metrics_diff['ttft']['difference_percent']
        if abs(ttft_diff) < 5:
            recommendations.append("‚úÖ TTFT: Impact n√©gligeable de la calibration (<5%)")
        elif abs(ttft_diff) < 15:
            recommendations.append("‚ö†Ô∏è TTFT: Impact mod√©r√© de la calibration (5-15%)")
        else:
            recommendations.append("‚ùå TTFT: Impact significatif de la calibration (>15%)")
    
    # Analyse Throughput
    if 'throughput' in metrics_diff:
        tps_diff = metrics_diff['throughput']['difference_percent']
        if abs(tps_diff) < 5:
            recommendations.append("‚úÖ Throughput: Impact n√©gligeable de la calibration (<5%)")
        elif abs(tps_diff) < 15:
            recommendations.append("‚ö†Ô∏è Throughput: Impact mod√©r√© de la calibration (5-15%)")
        else:
            recommendations.append("‚ùå Throughput: Impact significatif de la calibration (>15%)")
    
    # Analyse Warnings
    if warnings_analysis['warnings_resolved']:
        recommendations.append(f"‚úÖ Warnings FP8 r√©solus: {len(warnings_analysis['warnings_resolved'])}")
    
    if warnings_analysis['warnings_remaining']:
        recommendations.append(f"‚ö†Ô∏è Warnings restants: {len(warnings_analysis['warnings_remaining'])}")
    
    # Recommandation finale
    fp8_warnings_resolved = len(warnings_analysis['warnings_resolved']) >= 3  # Au moins 3 warnings FP8 r√©solus
    performance_impact_acceptable = True
    
    if fp8_warnings_resolved and performance_impact_acceptable:
        recommendations.append("üéØ RECOMMANDATION: Appliquer --calculate-kv-scales en production")
    elif fp8_warnings_resolved:
        recommendations.append("üîÑ RECOMMANDATION: Appliquer --calculate-kv-scales avec monitoring performance")
    else:
        recommendations.append("‚ùå RECOMMANDATION: Garder configuration actuelle (calibration inefficace)")
    
    report["recommendations"] = recommendations
    
    # Sauvegarder le rapport
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # Afficher r√©sum√©
    print("\nüìä R√âSUM√â DE LA COMPARAISON")
    print("=" * 50)
    
    if 'ttft' in metrics_diff:
        ttft = metrics_diff['ttft']
        print(f"TTFT: {ttft['baseline_ms']}ms ‚Üí {ttft['calibrated_ms']}ms ({ttft['difference_percent']:+.1f}%)")
    
    if 'throughput' in metrics_diff:
        tps = metrics_diff['throughput']
        print(f"Throughput: {tps['baseline_tps']} ‚Üí {tps['calibrated_tps']} tok/s ({tps['difference_percent']:+.1f}%)")
    
    print(f"Warnings r√©solus: {len(warnings_analysis['warnings_resolved'])}")
    print(f"Warnings restants: {len(warnings_analysis['warnings_remaining'])}")
    
    print("\nüéØ RECOMMANDATION FINALE:")
    for rec in recommendations:
        if "RECOMMANDATION" in rec:
            print(f"  {rec}")
    
    print(f"\nüìÑ Rapport d√©taill√© sauvegard√©: {output_file}")
    return True

def main():
    parser = argparse.ArgumentParser(description="Comparaison des benchmarks FP8")
    parser.add_argument("--baseline", required=True, help="Fichier benchmark baseline")
    parser.add_argument("--calibrated", required=True, help="Fichier benchmark calibr√©")
    parser.add_argument("--output", required=True, help="Fichier de sortie du rapport")
    
    args = parser.parse_args()
    
    success = generate_comparison_report(
        args.baseline,
        args.calibrated,
        args.output
    )
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()