#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script de test pour les services vLLM
Ce script permet de tester les trois configurations de vLLM (micro, mini, medium)
et leurs fonctionnalit√©s sp√©cifiques (outils, raisonnement, d√©codage sp√©culatif).
"""

import os
import time
import json
import logging
import argparse
import requests
from dotenv import load_dotenv
import asyncio
import aiohttp
import random
from openai import OpenAI
from typing import Dict, List, Optional, Tuple, Union, Any

# Configuration du logger avec couleurs
class ColorFormatter(logging.Formatter):
    """
    Un formatter color√© pour rendre les logs plus lisibles.
    """
    colors = {
        'DEBUG': '\033[94m',
        'INFO': '\033[92m',
        'WARNING': '\033[93m',
        'ERROR': '\033[91m',
        'CRITICAL': '\033[91m\033[1m'
    }
    reset = '\033[0m'

    def format(self, record):
        msg = super().format(record)
        return f"{self.colors.get(record.levelname, '')}{msg}{self.reset}"

logger = logging.getLogger("vLLM Tests")
logger.setLevel(logging.INFO)

if not logger.handlers:
    handler = logging.StreamHandler()
    handler.setLevel(logging.INFO)
    formatter = ColorFormatter(
        fmt="%(asctime)s [%(levelname)s] %(name)s - %(message)s",
        datefmt="%H:%M:%S"
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)

# Chargement des variables d'environnement
load_dotenv()

# Configuration des endpoints
endpoints = []

# Fonction pour charger les endpoints depuis le fichier .env
def load_endpoints():
    """
    Charge les endpoints depuis le fichier .env
    Format attendu:
    OPENAI_ENDPOINT_NAME_X=nom
    OPENAI_API_KEY_X=cl√©
    OPENAI_BASE_URL_X=url
    OPENAI_CHAT_MODEL_ID_X=mod√®le
    """
    global endpoints
    endpoints = []
    
    # Chercher les endpoints dans les variables d'environnement
    for i in range(1, 10):  # Chercher jusqu'√† 10 endpoints
        suffix = f"_{i}" if i > 1 else ""
        name = os.environ.get(f"OPENAI_ENDPOINT_NAME{suffix}")
        api_key = os.environ.get(f"OPENAI_API_KEY{suffix}")
        api_base = os.environ.get(f"OPENAI_BASE_URL{suffix}")
        model = os.environ.get(f"OPENAI_CHAT_MODEL_ID{suffix}")
        
        if name and api_key and api_base:
            endpoints.append({
                "name": name,
                "api_key": api_key,
                "api_base": api_base,
                "model": model
            })
    
    logger.info(f"Endpoints charg√©s: {[ep['name'] for ep in endpoints]}")
    return endpoints

# Test de connexion simple
def test_connection(endpoint):
    """
    Teste la connexion √† un endpoint en r√©cup√©rant la liste des mod√®les disponibles
    """
    logger.info(f"Test de connexion pour {endpoint['name']}...")
    
    try:
        url = f"{endpoint['api_base']}/models"
        headers = {
            "Authorization": f"Bearer {endpoint['api_key']}"
        }
        
        start_time = time.time()
        response = requests.get(url, headers=headers, timeout=10)
        elapsed = time.time() - start_time
        
        if response.status_code == 200:
            models = response.json()
            logger.info(f"  ‚úÖ Connexion r√©ussie √† {endpoint['name']} en {elapsed:.2f}s")
            logger.info(f"  üìã Mod√®les disponibles: {models.get('data', [])}")
            return True
        else:
            logger.error(f"  ‚ùå √âchec de connexion √† {endpoint['name']}: {response.status_code} - {response.text}")
            return False
    except Exception as e:
        logger.error(f"  ‚ùå Exception lors de la connexion √† {endpoint['name']}: {e}")
        return False

# Test de g√©n√©ration de texte simple
def test_text_generation(endpoint, prompt="Bonjour, comment vas-tu aujourd'hui?"):
    """
    Teste la g√©n√©ration de texte simple
    """
    logger.info(f"Test de g√©n√©ration de texte pour {endpoint['name']}...")
    
    client = OpenAI(api_key=endpoint['api_key'], base_url=endpoint['api_base'])
    
    try:
        start_time = time.time()
        response = client.chat.completions.create(
            model=endpoint.get('model', 'default'),
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=100
        )
        elapsed = time.time() - start_time
        
        content = response.choices[0].message.content
        tokens = response.usage.total_tokens if response.usage else None
        
        logger.info(f"  ‚úÖ G√©n√©ration r√©ussie en {elapsed:.2f}s")
        logger.info(f"  üìã R√©ponse: {content[:100]}...")
        if tokens:
            logger.info(f"  üìä Tokens: {tokens}, Vitesse: {tokens/elapsed:.2f} tokens/s")
        
        return True, content, elapsed, tokens
    except Exception as e:
        logger.error(f"  ‚ùå Exception lors de la g√©n√©ration de texte: {e}")
        return False, None, None, None

# Test d'utilisation d'outils
def test_tool_usage(endpoint):
    """
    Teste l'utilisation d'outils (function calling)
    """
    logger.info(f"Test d'utilisation d'outils pour {endpoint['name']}...")
    
    client = OpenAI(api_key=endpoint['api_key'], base_url=endpoint['api_base'])
    
    # D√©finition des outils
    tools = [
        {
            "type": "function",
            "function": {
                "name": "get_weather",
                "description": "Obtenir la m√©t√©o actuelle pour une ville",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "city": {
                            "type": "string",
                            "description": "La ville pour laquelle obtenir la m√©t√©o"
                        },
                        "unit": {
                            "type": "string",
                            "enum": ["celsius", "fahrenheit"],
                            "description": "L'unit√© de temp√©rature"
                        }
                    },
                    "required": ["city"]
                }
            }
        }
    ]
    
    try:
        start_time = time.time()
        response = client.chat.completions.create(
            model=endpoint.get('model', 'default'),
            messages=[
                {"role": "user", "content": "Quelle est la m√©t√©o √† Paris aujourd'hui?"}
            ],
            tools=tools,
            tool_choice="auto",
            max_tokens=200
        )
        elapsed = time.time() - start_time
        
        tool_calls = response.choices[0].message.tool_calls
        
        if tool_calls:
            logger.info(f"  ‚úÖ Utilisation d'outils r√©ussie en {elapsed:.2f}s")
            for tool_call in tool_calls:
                function_name = tool_call.function.name
                function_args = json.loads(tool_call.function.arguments)
                logger.info(f"  üìã Fonction appel√©e: {function_name}")
                logger.info(f"  üìã Arguments: {function_args}")
            return True
        else:
            logger.warning(f"  ‚ö†Ô∏è Pas d'appel d'outil d√©tect√© dans la r√©ponse")
            logger.info(f"  üìã Contenu: {response.choices[0].message.content}")
            return False
    except Exception as e:
        logger.error(f"  ‚ùå Exception lors du test d'outils: {e}")
        return False

# Test de raisonnement
def test_reasoning(endpoint):
    """
    Teste la capacit√© de raisonnement du mod√®le
    """
    logger.info(f"Test de raisonnement pour {endpoint['name']}...")
    
    client = OpenAI(api_key=endpoint['api_key'], base_url=endpoint['api_base'])
    
    prompt = """
    R√©sous ce probl√®me √©tape par √©tape:
    
    Jean a 5 pommes. Marie lui en donne 3 de plus. 
    Jean mange 2 pommes puis donne la moiti√© des pommes restantes √† Pierre.
    Combien de pommes Jean a-t-il maintenant?
    """
    
    try:
        start_time = time.time()
        response = client.chat.completions.create(
            model=endpoint.get('model', 'default'),
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=300
        )
        elapsed = time.time() - start_time
        
        content = response.choices[0].message.content
        
        logger.info(f"  ‚úÖ Test de raisonnement termin√© en {elapsed:.2f}s")
        logger.info(f"  üìã R√©ponse: {content}")
        
        # V√©rifier si la r√©ponse contient "3 pommes" qui est la bonne r√©ponse
        if "3 pommes" in content.lower():
            logger.info(f"  ‚úÖ La r√©ponse semble correcte (contient '3 pommes')")
            return True
        else:
            logger.warning(f"  ‚ö†Ô∏è La r√©ponse ne contient pas explicitement '3 pommes'")
            return False
    except Exception as e:
        logger.error(f"  ‚ùå Exception lors du test de raisonnement: {e}")
        return False

# Benchmark de performance
def benchmark_performance(endpoint, prompt=None, repeats=3):
    """
    Effectue un benchmark de performance
    """
    if prompt is None:
        prompt = (
            "R√©dige un texte d'environ 500 mots sur l'IA, "
            "en √©voquant l'apprentissage machine, les grands mod√®les de langage, "
            "et quelques perspectives d'√©volution."
        )
    
    logger.info(f"Benchmark de performance pour {endpoint['name']} ({repeats} r√©p√©titions)...")
    
    client = OpenAI(api_key=endpoint['api_key'], base_url=endpoint['api_base'])
    
    # Warm-up
    logger.info("  üîÑ Warm-up...")
    try:
        client.chat.completions.create(
            model=endpoint.get('model', 'default'),
            messages=[
                {"role": "user", "content": "Warm up. Ignorez ce message."}
            ],
            max_tokens=10
        )
    except Exception as e:
        logger.warning(f"  ‚ö†Ô∏è √âchec du warm-up: {e}")
    
    total_time = 0
    total_tokens = 0
    success_count = 0
    
    for i in range(repeats):
        logger.info(f"  üîÑ It√©ration {i+1}/{repeats}")
        
        try:
            start_time = time.time()
            response = client.chat.completions.create(
                model=endpoint.get('model', 'default'),
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500
            )
            elapsed = time.time() - start_time
            
            tokens = response.usage.total_tokens if response.usage else None
            
            logger.info(f"    ‚è±Ô∏è Dur√©e: {elapsed:.2f}s, tokens: {tokens}")
            
            total_time += elapsed
            if tokens:
                total_tokens += tokens
            success_count += 1
            
        except Exception as e:
            logger.error(f"    ‚ùå Exception: {e}")
    
    if success_count == 0:
        logger.error(f"  ‚ùå Toutes les it√©rations ont √©chou√©")
        return None
    
    avg_time = total_time / success_count
    tokens_per_sec = total_tokens / total_time if total_time > 0 else 0
    
    logger.info(f"  üìä R√©sultats du benchmark:")
    logger.info(f"    ‚è±Ô∏è Temps moyen: {avg_time:.2f}s")
    logger.info(f"    üìà Vitesse: {tokens_per_sec:.2f} tokens/s")
    
    return {
        "avg_time": avg_time,
        "tokens_per_sec": tokens_per_sec,
        "total_tokens": total_tokens,
        "success_count": success_count
    }

# Test de traitement parall√®le (batching)
async def test_parallel_processing(endpoint, n_parallel=5):
    """
    Teste le traitement parall√®le (batching)
    """
    logger.info(f"Test de traitement parall√®le pour {endpoint['name']} ({n_parallel} requ√™tes)...")
    
    prompt = "Bonjour, ceci est un test de requ√™tes parall√®les. Peux-tu me donner quelques id√©es cr√©atives pour un week-end ?"
    
    async def async_chat_completion(api_base, api_key, model, prompt):
        url = f"{api_base}/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }
        payload = {
            "model": model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 200
        }
        
        async with aiohttp.ClientSession() as session:
            start_t = time.time()
            try:
                async with session.post(url, headers=headers, json=payload, timeout=60) as resp:
                    elapsed = time.time() - start_t
                    if resp.status == 200:
                        data = await resp.json()
                        tokens = None
                        if "usage" in data and data["usage"].get("total_tokens"):
                            tokens = data["usage"]["total_tokens"]
                        return (elapsed, tokens)
                    else:
                        return (None, None)
            except Exception as e:
                logger.error(f"  ‚ùå Exception asynchrone: {e}")
                return (None, None)
    
    tasks = []
    for _ in range(n_parallel):
        # Ajouter un pr√©fixe al√©atoire pour √©viter le cache
        prefix = ''.join(random.choices('ABCDEFGHIJKLMNOPQRSTUVWXYZ', k=5))
        modified_prompt = f"{prefix} {prompt}"
        
        tasks.append(asyncio.create_task(
            async_chat_completion(
                endpoint["api_base"], 
                endpoint["api_key"], 
                endpoint.get("model", "default"), 
                modified_prompt
            )
        ))
    
    start_time = time.time()
    results = await asyncio.gather(*tasks)
    total_time = time.time() - start_time
    
    nb_ok = 0
    sum_tokens = 0
    for (elapsed, tokens) in results:
        if elapsed is not None and tokens is not None:
            nb_ok += 1
            sum_tokens += tokens
    
    logger.info(f"  üìä R√©sultats du test parall√®le:")
    logger.info(f"    ‚úÖ {nb_ok}/{n_parallel} requ√™tes r√©ussies")
    logger.info(f"    ‚è±Ô∏è Dur√©e totale: {total_time:.2f}s")
    logger.info(f"    üìà Tokens cumul√©s: {sum_tokens}")
    
    speed = sum_tokens / total_time if total_time > 0 else 0
    logger.info(f"    üìà Vitesse globale: {speed:.2f} tokens/s")
    
    return {
        "success_count": nb_ok,
        "total_time": total_time,
        "total_tokens": sum_tokens,
        "tokens_per_sec": speed
    }

# Fonction principale
async def main():
    parser = argparse.ArgumentParser(description="Tests des services vLLM")
    parser.add_argument("--connection", action="store_true", help="Tester la connexion")
    parser.add_argument("--generation", action="store_true", help="Tester la g√©n√©ration de texte")
    parser.add_argument("--tools", action="store_true", help="Tester l'utilisation d'outils")
    parser.add_argument("--reasoning", action="store_true", help="Tester le raisonnement")
    parser.add_argument("--benchmark", action="store_true", help="Effectuer un benchmark de performance")
    parser.add_argument("--parallel", action="store_true", help="Tester le traitement parall√®le")
    parser.add_argument("--all", action="store_true", help="Ex√©cuter tous les tests")
    parser.add_argument("--repeats", type=int, default=3, help="Nombre de r√©p√©titions pour le benchmark")
    parser.add_argument("--parallel-requests", type=int, default=5, help="Nombre de requ√™tes parall√®les")
    
    args = parser.parse_args()
    
    # Si aucun test n'est sp√©cifi√©, ex√©cuter tous les tests
    if not (args.connection or args.generation or args.tools or args.reasoning or args.benchmark or args.parallel):
        args.all = True
    
    # Charger les endpoints
    load_endpoints()
    
    if not endpoints:
        logger.error("‚ùå Aucun endpoint trouv√© dans le fichier .env")
        return
    
    # R√©sultats des tests
    results = {ep["name"]: {} for ep in endpoints}
    
    # Ex√©cuter les tests pour chaque endpoint
    for endpoint in endpoints:
        logger.info(f"\n=== Tests pour {endpoint['name']} ===\n")
        
        # Test de connexion
        if args.connection or args.all:
            connection_ok = test_connection(endpoint)
            results[endpoint["name"]]["connection"] = connection_ok
            
            # Si la connexion √©choue, passer √† l'endpoint suivant
            if not connection_ok:
                logger.error(f"‚ùå Impossible de se connecter √† {endpoint['name']}, tests suivants ignor√©s")
                continue
        
        # Test de g√©n√©ration de texte
        if args.generation or args.all:
            gen_ok, _, _, _ = test_text_generation(endpoint)
            results[endpoint["name"]]["generation"] = gen_ok
        
        # Test d'utilisation d'outils
        if args.tools or args.all:
            tools_ok = test_tool_usage(endpoint)
            results[endpoint["name"]]["tools"] = tools_ok
        
        # Test de raisonnement
        if args.reasoning or args.all:
            reasoning_ok = test_reasoning(endpoint)
            results[endpoint["name"]]["reasoning"] = reasoning_ok
        
        # Benchmark de performance
        if args.benchmark or args.all:
            benchmark_results = benchmark_performance(endpoint, repeats=args.repeats)
            results[endpoint["name"]]["benchmark"] = benchmark_results
        
        # Test de traitement parall√®le
        if args.parallel or args.all:
            parallel_results = await test_parallel_processing(endpoint, n_parallel=args.parallel_requests)
            results[endpoint["name"]]["parallel"] = parallel_results
    
    # Afficher le r√©sum√© des tests
    logger.info("\n=== R√©sum√© des tests ===\n")
    
    for name, result in results.items():
        logger.info(f"Endpoint: {name}")
        
        if "connection" in result:
            status = "‚úÖ" if result["connection"] else "‚ùå"
            logger.info(f"  Connexion: {status}")
        
        if "generation" in result:
            status = "‚úÖ" if result["generation"] else "‚ùå"
            logger.info(f"  G√©n√©ration de texte: {status}")
        
        if "tools" in result:
            status = "‚úÖ" if result["tools"] else "‚ùå"
            logger.info(f"  Utilisation d'outils: {status}")
        
        if "reasoning" in result:
            status = "‚úÖ" if result["reasoning"] else "‚ùå"
            logger.info(f"  Raisonnement: {status}")
        
        if "benchmark" in result and result["benchmark"]:
            logger.info(f"  Benchmark: {result['benchmark']['tokens_per_sec']:.2f} tokens/s")
        
        if "parallel" in result and result["parallel"]:
            logger.info(f"  Traitement parall√®le: {result['parallel']['tokens_per_sec']:.2f} tokens/s")
    
    logger.info("\nTests termin√©s.")

if __name__ == "__main__":
    asyncio.run(main())