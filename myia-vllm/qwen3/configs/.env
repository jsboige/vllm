# Fichier d'environnement pour vLLM
# Généré le $(Get-Date -Format "yyyy-MM-dd HH:mm:ss")

# Token Hugging Face (requis pour accéder aux modèles)
# Utilisez un token de test pour le déploiement initial
HUGGING_FACE_HUB_TOKEN=YOUR_HUGGING_FACE_TOKEN_HERE
HF_TOKEN=YOUR_HUGGING_FACE_TOKEN_HERE

# Clés API pour les différents services
# Utilisez des clés simples pour le test initial
VLLM_API_KEY_MICRO=test-key-micro
VLLM_API_KEY_MINI=test-key-mini
VLLM_API_KEY_MEDIUM=test-key-medium

# Ports pour les différents services
VLLM_PORT_MICRO=5000
VLLM_PORT_MINI=5001
VLLM_PORT_MEDIUM=5002

# Configuration des GPUs
# Format: numéro(s) de GPU séparés par des virgules
CUDA_VISIBLE_DEVICES_MICRO=2
CUDA_VISIBLE_DEVICES_MINI=2
CUDA_VISIBLE_DEVICES_MEDIUM=0,1

# Chemin vers le cache Hugging Face
# Adaptez selon votre environnement
HF_CACHE_PATH=\\wsl.localhost\Ubuntu\home\jesse\vllm\.cache\huggingface\hub
HF_TOKEN=YOUR_HUGGING_FACE_TOKEN_HERE

# Fuseau horaire
TZ=Europe/Paris

# Paramètres d'utilisation de la mémoire GPU
GPU_MEMORY_UTILIZATION_MICRO=0.9999
GPU_MEMORY_UTILIZATION_MINI=0.9999
GPU_MEMORY_UTILIZATION_MEDIUM=0.9999

# Permettre des longueurs de contexte plus grandes que celles définies dans les modèles
VLLM_ALLOW_LONG_MAX_MODEL_LEN=1
